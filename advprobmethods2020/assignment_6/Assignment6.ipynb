{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82adb542d407e2bca331c0144d3a1f58",
     "grade": false,
     "grade_id": "cell-959673e166ae23a4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2020)\n",
    "Pekka Marttinen, Santosh Hiremath, Marko Järvenpää, Tianyu Cui, Yogesh Kumar, Diego Mesquita, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Joakim Järvinen.\n",
    "\n",
    "## Assignment 6, due on Tuesday,  10th March at 23:55.\n",
    "\n",
    "### Contents\n",
    "1. [Problem 1: Deriving VB for a simple model (1/2)](#Problem-1:-Deriving-VB-for-a-simple-model-(1/2))\n",
    "2. [Problem 2: Deriving VB for a simple model (2/2)](#Problem-2:-Deriving-VB-for-a-simple-model-(2/2))\n",
    "3. [Problem 3: KL-divergence](#Problem-3:-KL-divergence)\n",
    "4. [Problem 4: Variational approximation for a simple distribution](#Problem-4:-Variational-approximation-for-a-simple-distribution)\n",
    "\n",
    "\n",
    "# Problem 1: Deriving VB for a simple model (1/2)\n",
    "Consider the variational Bayesian approximation for the example model from the lecture (see 'simple_vb_example.pdf' in the materials of lecture 6). Derive the VB update for the factor $ q(\\tau)$ in the example and complete the code block 'Problem 1' in the template given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 1: Derivation of $q(\\tau)$ \n",
    "Write your solution to Problem 1 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c57682e00e81360524d88566f9a8cf3",
     "grade": false,
     "grade_id": "cell-63ddaadba3119b76",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Deriving VB for a simple model (2/2)\n",
    "As in Problem 1, consider the variational Bayesian approximation for the example model from the lecture (simple vb example.pdf). Now, derive the VB update for the factor $q(\\theta)$ in the example and complete the code block 'Problem 2’ in the template below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to problem 2: Derivation of $q(\\theta)$ \n",
    "Write your solution to Problem 2 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec888386f5e4f7fee89b890dcc71505",
     "grade": false,
     "grade_id": "cell-98ac851176e67ace",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problems 1 and 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, beta\n",
    "\n",
    "np.random.seed(123123123)\n",
    "\n",
    "# Simulate data\n",
    "theta_true = 4\n",
    "tau_true = 0.3\n",
    "n_samples = 10000\n",
    "z = (np.random.rand(n_samples) < tau_true)  # True with probability tau_true\n",
    "x = np.random.randn(n_samples) + z * theta_true\n",
    "\n",
    "# Parameters of the prior distributions.\n",
    "alpha0 = 0.5\n",
    "beta0 = 0.2\n",
    "\n",
    "# The number of iterations\n",
    "n_iter = 15\n",
    "\n",
    "# Some initial value for the things that will be updated\n",
    "E_log_tau = -0.7   # E(log(tau))\n",
    "E_log_tau_c = -0.7  # E(log(1-tau))\n",
    "E_log_var = 4 * np.ones(n_samples)  # E((x_n-theta)^2)\n",
    "r2 = 0.5 * np.ones(n_samples)  # Responsibilities of the second cluster.\n",
    "\n",
    "# init the plot\n",
    "iters_to_plot = [0, 2, 14]\n",
    "fig, ax = plt.subplots(3, len(iters_to_plot), figsize=(10, 8), sharex='row', sharey='row')\n",
    "col = 0 # plot column\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Updated of responsibilites, factor q(z)\n",
    "    log_rho1 = E_log_tau_c - 0.5 * np.log(2 * np.pi) - 0.5 * (x ** 2)\n",
    "    log_rho2 = E_log_tau - 0.5 * np.log(2 * np.pi) - 0.5 * E_log_var\n",
    "    max_log_rho = np.maximum(log_rho1, log_rho2)  # Normalize to avoid numerical problems when exponentiating.\n",
    "    rho1 = np.exp(log_rho1 - max_log_rho)\n",
    "    rho2 = np.exp(log_rho2 - max_log_rho)\n",
    "    r2 = rho2 / (rho1 + rho2)\n",
    "    r1 = 1 - r2\n",
    "    \n",
    "    N1 = np.sum(r1)\n",
    "    N2 = np.sum(r2)\n",
    "    \n",
    "    # ====== Problem 1 =======================\n",
    "    # Update of factor q(tau)    \n",
    "    # E_log_tau = ? # EXERCISE\n",
    "    # E_log_tau_c = ? # EXERCISE\n",
    "\n",
    "    # Current estimate of tau\n",
    "    # tau_est = ? (mean of q(tau))    \n",
    "    #=========================================\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "     \n",
    "    # ====== Problem 2 =======================\n",
    "    # Update of factor q(theta)\n",
    "    # E_log_var = ? #EXERCISE\n",
    "\n",
    "    # Current estimate theta\n",
    "    # theta_est = ? #EXERCISE\n",
    "    #=========================================\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # plotting\n",
    "    if i in iters_to_plot:\n",
    "        # plot estimated data distribution\n",
    "        xgrid = np.linspace(-4, 8, 100)\n",
    "        ax[0,col].hist(x, xgrid, label=\"data histogram\")\n",
    "        pdf_true = (1-tau_true) * norm.pdf(xgrid, 0, 1) + tau_true * norm.pdf(xgrid, theta_true, 1)\n",
    "        pdf_est = (1-tau_est) * norm.pdf(xgrid, 0, 1) + tau_est * norm.pdf(xgrid, theta_est, 1)\n",
    "        ax[0,col].plot(xgrid, pdf_true * n_samples * (xgrid[1]-xgrid[0]), 'k', label=\"true pdf\")\n",
    "        ax[0,col].plot(xgrid, pdf_est * n_samples * (xgrid[1]-xgrid[0]), 'r', label=\"estimated pdf\")\n",
    "        if i == 0:\n",
    "            ax[0,i].legend()\n",
    "        ax[0,col].set_title((\"After %d iterations\\n\" +\n",
    "                            \"($\\\\mathrm{E}_q[\\\\tau]$=%.3f, $\\\\mathrm{E}_q[\\\\theta]$=%.3f)\") %\n",
    "                            (i + 1, tau_est, theta_est))\n",
    "        ax[0,col].set_xlabel(\"$x$\")\n",
    "        \n",
    "        # plot marginal distribution of tau\n",
    "        tau = np.linspace(0, 1.0, 1000)\n",
    "        q_tau = beta.pdf(tau, N2 + alpha0, N1 + alpha0)\n",
    "        ax[1,col].plot(tau, q_tau)\n",
    "        ax[1,col].set_xlabel(\"$\\\\tau$\")\n",
    "        \n",
    "        # plot marginal distribution of theta\n",
    "        theta = np.linspace(-4.0, 8.0, 1000)\n",
    "        q_theta = norm.pdf(theta, m2, 1.0)\n",
    "        ax[2,col].plot(theta, q_theta)\n",
    "        ax[2,col].set_xlabel(\"$\\\\theta$\")\n",
    "        col = col + 1\n",
    "\n",
    "# finalize the plot\n",
    "ax[1,0].set_ylabel(\"$q(\\\\tau)$\")\n",
    "ax[2,0].set_ylabel(\"$q(\\\\theta)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20337897840a9e33d62fa71349e20492",
     "grade": false,
     "grade_id": "cell-cd1d2c1fd1c2286d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 3: KL-divergence  \n",
    "Recall the Normal-Gamma posterior example from lecture 3. Your task is to compute the KL-divergence between the true distribution of the samples and the distribution estimated using Bayesian learning. Repeat the computation for training set sizes in the range 5–5000 and as a final output, plot the KL-divergence as a function of the training set size. \n",
    "\n",
    "You can use the template below as a starting point. You will need to write the computation of the KL-divergence between the true and learned distributions and plot the results (you may remove the existing plots, as they are not needed for this exercise). More hints are given in the template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e721c7dbbeb91814c69bffa21ae6fac",
     "grade": false,
     "grade_id": "cell-e42736ef941630e8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Template for problem 3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma, norm\n",
    "\n",
    "np.random.seed(91) # Set random number generator.\n",
    "\n",
    "kl_div = []\n",
    "\n",
    "for num_samples in range(5, 5001, 100):\n",
    "    # SIMULATE THE TRUE DATA SET\n",
    "\n",
    "    lambda_true = 4   # precision\n",
    "    mu_true = 2       # mean\n",
    "    sigma_true = 1 / np.sqrt(lambda_true)   # standard deviation\n",
    "    data_set = np.random.normal(mu_true, sigma_true, num_samples)\n",
    "\n",
    "    # SPECIFY PRIORS\n",
    "\n",
    "    # lambda is the precision parameter of the unknown Gaussian\n",
    "    # and it is given a prior distribution Gamma(a0,b0),\n",
    "    # (a0 is the 'shape' and b0 the 'rate')\n",
    "    a0 = 0.01\n",
    "    b0 = 0.01 # These correspond to a noninformative prior\n",
    "\n",
    "    # mu is the mean parameter of the unknown Gaussian\n",
    "    # and it is given a prior distribution that depends on\n",
    "    # lambda: N(mu0, (beta0*lambda)^-1)\n",
    "    mu0 = 0\n",
    "    beta0 = 0.001 # Low precision corresponds to high variance\n",
    "\n",
    "    # (This is the so-called Normal-Gamma(mu0, beta0, a0, b0)\n",
    "    # prior distribution for mu and lambda)\n",
    "\n",
    "\n",
    "    # LEARN THE POSTERIOR DISTRIBUTION\n",
    "\n",
    "    # Due to conjugacy, the posterior distribution is also\n",
    "    # Normal-Gamma(mu_n, beta_n, a_n, b_n)\n",
    "\n",
    "    sample_mean = sum(data_set) / num_samples;\n",
    "    sample_var = sum((data_set - sample_mean)**2) / num_samples;\n",
    "\n",
    "    a_n = a0 + num_samples / 2\n",
    "\n",
    "    b_n = b0 + (num_samples * sample_var + (beta0 * num_samples * (sample_mean-mu0)**2) / (beta0 + num_samples)) / 2\n",
    "\n",
    "    mu_n = (mu0 * beta0 + num_samples * sample_mean) / (beta0 + num_samples)\n",
    "\n",
    "    beta_n = beta0 + num_samples\n",
    "\n",
    "\n",
    "    # Plot distribution of lambda, the precision\n",
    "    lambda_range = np.arange(0, 10, 0.01)\n",
    "    prior_lambda_pdf     = gamma.pdf(lambda_range, a0,  scale=1/b0)\n",
    "    posterior_lambda_pdf = gamma.pdf(lambda_range, a_n, scale=1/b_n)\n",
    "\n",
    "    # Plot distribution of mu, the mean\n",
    "    mu_range = np.arange(1, 3, 0.01)\n",
    "    # Because mu depends on lambda, we need to integrate over \n",
    "    # lambda. We do this by Monte Carlo integration (i.e. \n",
    "    # average over multiple simulated lambdas)\n",
    "    gamma_prior_samples = np.random.gamma(a0, 1/b0, 100)\n",
    "\n",
    "    sum_prior_mu_pdf = np.zeros(len(mu_range))\n",
    "    for gamma_sample in gamma_prior_samples:\n",
    "        prior_mu_pdf = norm.pdf(mu_range, mu0, 1 / np.sqrt((beta0 * gamma_sample)))\n",
    "        sum_prior_mu_pdf += prior_mu_pdf\n",
    "\n",
    "    prior_mu_pdf = sum_prior_mu_pdf / len(gamma_prior_samples)\n",
    "\n",
    "    gamma_posterior_samples = np.random.gamma(a_n, 1/b_n, 100)\n",
    "\n",
    "    sum_posterior_mu_pdf = np.zeros(len(mu_range))\n",
    "    for gamma_sample in gamma_posterior_samples:\n",
    "        posterior_mu_pdf = norm.pdf(mu_range, mu_n, 1 / np.sqrt(beta_n * gamma_sample))\n",
    "        sum_posterior_mu_pdf += posterior_mu_pdf\n",
    "\n",
    "    posterior_mu_pdf = sum_posterior_mu_pdf / len(gamma_prior_samples)\n",
    "\n",
    "    # plt.plot(mu_range, prior_mu_pdf, label=\"prior\")\n",
    "    # plt.plot(mu_range, posterior_mu_pdf, label=\"posterior\")\n",
    "    # plt.plot([mu_true,mu_true],[0,2.5], \"k-\", label=\"true value\")\n",
    "    # plt.title('mu')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # We estimate the parameters with the mean of the posterior distribution\n",
    "    mu_hat     = sum(posterior_mu_pdf     * mu_range)     / sum(posterior_mu_pdf)\n",
    "    lambda_hat = sum(posterior_lambda_pdf * lambda_range) / sum(posterior_lambda_pdf)\n",
    "\n",
    "    full_dist_range = np.arange(-2, 6, 0.1)\n",
    "    true_pdf      = norm.pdf(full_dist_range, mu_true, sigma_true)\n",
    "    estimated_pdf = norm.pdf(full_dist_range, mu_hat,  1 / np.sqrt(lambda_hat))\n",
    "    \n",
    "    # plt.plot(full_dist_range, true_pdf, label=\"true\")\n",
    "    # plt.plot(full_dist_range, estimated_pdf, label=\"estimated\")\n",
    "    # plt.title('Distribution of the samples')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # ====== Problem 3 =======================\n",
    "    # COMPUTE K-L DIVERGENCE BETWEEN TRUE AND ESTIMATED SAMPLE DISTRIBUTIONS \n",
    "    # (two alternative numerical integration techniques)\n",
    "    \n",
    "    # Hints: \n",
    "    # For computing the KL-divergence, use numerical integration over a grid of \n",
    "    # values. \"full_dist_range\" specifies a suitable grid along the x-axis.\n",
    "    # Values of the true PDF estimated at the grid points are given in \n",
    "    # \"true_pdf\" and values of the estimated PDF at the grid points are given\n",
    "    # in \"estimated_pdf\". For computing the integral, you can use any numerical\n",
    "    # integration available in Numpy, e.g., the \"trapz\" function.\n",
    "    # ========================================\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d125c3609e5c327ded4e67d2133354f",
     "grade": true,
     "grade_id": "cell-87603a519775ec61",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5be5afe23465f4c7de6d88e1a84326e2",
     "grade": false,
     "grade_id": "cell-dffc08e55deb0f56",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 4: Variational approximation for a simple distribution\n",
    "Consider a model with two binary random variables $x_1 and $x_2, defined by the distributions:\n",
    "\n",
    "$p(x_1=0) = 0.4$\n",
    "\n",
    "$p(x_2=0|x_1=0) = 0.5$, \n",
    "\n",
    "$p(x_2=1|x_1=0) = 0.5$\n",
    "\n",
    "$p(x_2=0|x_1=1) = 0.9$, \n",
    "\n",
    "$p(x_2=1|x_1=1) = 0.1$\n",
    "\n",
    "Find a fully factorized distribution $q(x_1, x_2) = q_1(x_1)q_2(x_2)$ that best approximates the joint $p(x_1, x_2)$, in the sense of minimizing $KL(p || q )$ .\n",
    "\n",
    "**Note**: For “normal” variational inference, we would rather minimize $KL(q || p )$; recall that, in general, $KL(p || q ) \\neq KL(q || p )$ (see Barber: Bayesian Reasoning and Machine Learning, ch. Figure 28.1 as well as Chapter 28.3.4 and 28.3.5, for the dramatically different solutions that can result by minimizing the different quantities, as well as commentary on their relative usefulness for approximate inference). Here, we’ll minimize $KL(p || q )$ , as that is algebraically simpler.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Problem 4 \n",
    "Write your solution to Problem 4 in LateX or attach a picture here. You can add a picture using the command ```!(imagename_in_folder.jpg)```. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
