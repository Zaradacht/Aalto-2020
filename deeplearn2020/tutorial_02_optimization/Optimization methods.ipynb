{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization methods. Effect of input normalization\n",
    "\n",
    "In this tutorial, we get familiar with optimization methods:\n",
    "- gradient descent\n",
    "- Newton's method\n",
    "- momentum momentum\n",
    "\n",
    "We will also see how input normalization can affect the difficulty of the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(fun, w1, w2, levels=None, linestyles=None):\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    F = np.array([\n",
    "        fun(np.array([_w1, _w2]))[0]\n",
    "        for _w1, _w2 in zip(W1.flat, W2.flat)\n",
    "    ]).reshape(W1.shape)\n",
    "    CS = ax.contour(W1, W2, F, levels=levels, linestyles=linestyles)\n",
    "    ax.set_ylabel('$w_2$')\n",
    "    ax.set_xlabel('$w_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny example\n",
    "\n",
    "## Data\n",
    "\n",
    "Suppose we have a dataset that contains two examples:\n",
    "$x_1 = (2, 2)$, $y_1 = 2$,\n",
    "$x_2 = (2, 0)$, $y_2 =0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((\n",
    "    np.array([2, 2]),\n",
    "    np.array([2, 0]),\n",
    "))\n",
    "y = np.array([2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a simple linear neuron with two inputs:\n",
    "$$\n",
    "f(x) = w^T x = w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "We can estimate the parameters of the model by maximizing the likelihood:\n",
    "$$\n",
    "  w_1, w_2 \\leftarrow \\arg \\min\n",
    "  \\sum_{i=1}^2 \\left(y_i - f(x_i) \\right)^2\n",
    "$$\n",
    "which is the problem of optimizing a quadratic function:\n",
    "$$\n",
    "  c(w) = \\frac{1}{2} w^T A w - b^T w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fn(w, x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      w (array): Array of weights of size (2)\n",
    "    \"\"\"\n",
    "    c = 0.5 * np.square(y - np.dot(w, x)).sum()\n",
    "    g = -np.dot(x, (y - np.dot(w, x)))\n",
    "    H = np.dot(x, x.T)\n",
    "    return c, g, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.arange(-2, 2, 0.01)\n",
    "w2 = np.arange(-1, 3, 0.01)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "## Gradient\n",
    "\n",
    "Let us first visualize the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = np.arange(-2, 2+0.01, 0.25)\n",
    "w2g = np.arange(-1, 3+0.01, 0.25)\n",
    "W1g, W2g = np.meshgrid(w1g, w2g)\n",
    "\n",
    "G = np.array([\n",
    "    obj_fn(np.array([_w1, _w2]), x, y)[1]\n",
    "    for _w1, _w2 in zip(W1g.flat, W2g.flat)\n",
    "])  # (N*M, 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "ax.quiver(W1g.flat, W2g.flat, G[:, 0], G[:, 1], scale=50, scale_units='xy', angles='xy')\n",
    "ax.set_xlim([w1[0], w1[-1]])\n",
    "ax.set_ylim([w2[0], w2[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the function, we need to go in the direction opposite to the gradient. Let us visualize that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative gradient\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "ax.quiver(W1g.flat, W2g.flat, -G[:, 0], -G[:, 1], scale=50, scale_units='xy', angles='xy')\n",
    "ax.set_xlim([w1[0], w1[-1]])\n",
    "ax.set_ylim([w2[0], w2[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "We update the parameters in the direction opposite to the gradient:\n",
    "$$\n",
    "  w \\leftarrow w - \\eta \\nabla_f (w)\n",
    "$$\n",
    "where $\\eta$ is the step size (learning rate).\n",
    "\n",
    "We reduce the error but do not end up at the minimum, so we need to iterate\n",
    "$$\n",
    "  w_{t+1} = w_t - \\eta_t \\nabla_f (w_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.17\n",
    "for i in range(100):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    dw = -lrate * g\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note:\n",
    "\n",
    "For a quadratic function, we can find the minimum in one step using Newton's method:\n",
    "$$\n",
    "w_{t+1} = w_t - H_t^{-1} \\nabla_f(w_t)\n",
    "$$\n",
    "where $H$ is the Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.1\n",
    "for i in range(20):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    dw = -np.linalg.solve(H, g)\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with a small learning rate\n",
    "\n",
    "With a small learning rate, the optimization process may be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.05\n",
    "for i in range(200):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    dw = -lrate * g\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with a large learning rate\n",
    "\n",
    "With a large learning rate, the trajectory oscillates and the optimization process can even diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.19\n",
    "for i in range(20):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    dw = -lrate * g\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum method\n",
    "\n",
    "In the momentum method, we aggregate negative gradients in momentum $m$ which is then used to update the parameters:\n",
    "\\begin{align*}\n",
    "m_{t+1} &= \\alpha m_t - \\eta_t g_t\n",
    "\\\\\n",
    "w_{t+1} &= w_t + m_{t+1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process becomes more consistent, the trajectory oscillates less.\n",
    "\n",
    "Let us try $\\alpha=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.5\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.17\n",
    "m = np.zeros(2)\n",
    "alpha = 0.5\n",
    "for i in range(100):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    m = dw = alpha * m -lrate * g\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inrease $\\alpha$ to 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.85\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "lrate = 0.1\n",
    "m = np.zeros(2)\n",
    "alpha = 0.85\n",
    "for i in range(100):\n",
    "    c, g, H = obj_fn(w, x, y)\n",
    "    m = dw = alpha * m -lrate * g\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the momentum method: A ball moving on the error surface: The location of the ball\n",
    "represents the value of the parameters ($w_1$, $w_2$)\n",
    "\n",
    "- At $t=0$, the ball follows the gradient\n",
    "\n",
    "- Once it has velocity, it no longer does steepest descent. Its momentum makes it keep going in the previous direction.\n",
    "\n",
    "The momentum method damps oscillations in directions of high curvature (by combining\n",
    "gradients with opposite signs) and it builds up speed in directions with a gentle but consistent gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimization of a non-quadratic los function\n",
    "\n",
    "Let us now consider a slightly more complex model: two layers with one neuron in each layer:\n",
    "\\begin{align*}\n",
    "f(x) &= w_2 w_1 x\n",
    "\\end{align*}\n",
    "\n",
    "The dataset contains one example: $\\{x=1, y=1.5\\}$\n",
    "\n",
    "We add weight decay regularization which yields the objective function\n",
    "$$\n",
    "  c(w_1, w_2) = (w_1 w_2 - 1.5)^2+0.04(w_1^2+w_2^2)\n",
    "$$\n",
    "We need to minimize $c$ wrt $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fn(w):\n",
    "    decay = 0.04\n",
    "    f = np.square(w[0] * w[1] - 1.5) + decay * np.square(w).sum()\n",
    "    g = np.array([\n",
    "        2 * (w[0] * w[1] - 1.5) * w[1] + 2 * decay * w[0],\n",
    "        2 * (w[0] * w[1] - 1.5) * w[0] + 2 * decay * w[1],\n",
    "    ])\n",
    "    H = np.array([\n",
    "        [2*w[1]*w[1] + 2*decay,      2*(2*w[0]*w[1] - 1.5)],\n",
    "        [2*(2*w[0]*w[1] - 1.5),      2 * w[0] * w[0] + 2 * decay]\n",
    "    ])\n",
    "    return f, g, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a contour plot of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "w1 = np.arange(-5, 5, 0.1)\n",
    "w2 = np.arange(-5, 5, 0.1)\n",
    "contour_plot(obj_fn, w1, w2, levels=np.arange(0.15, 1.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the same plot in 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "F = np.array([\n",
    "    obj_fn(np.array([_w1, _w2]))[0]\n",
    "    for _w1, _w2 in zip(W1.flat, W2.flat)\n",
    "]).reshape(W1.shape)\n",
    "surf = ax.plot_surface(W1, W2, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see two equivalent local minima because $c(w_1, w_2) = c(-w_1, -w_2)$.\n",
    "\n",
    "Note: The loss function that is minimized when training deep neural networks contains multiple local minima. Many of them are equivalent because the same loss can be obtained by swapping any pair of neurons in the same layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic approximation\n",
    "\n",
    "Let us zoom in to one of the local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "w1 = np.arange(0, 5, 0.1)\n",
    "w2 = np.arange(0, 5, 0.1)\n",
    "contour_plot(obj_fn, w1, w2, levels=np.arange(0.15, 1.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally the error surface is well approximated by a quadratic function:\n",
    "$$\n",
    "c(w) \\approx c(w_t) + \\nabla^T (w - w_t) + \\frac{1}{2}(w - w_t)^T H (w - w_t).\n",
    "$$\n",
    "where $H$ is the matrix of second-order derivatives (called Hessian):\n",
    "\\begin{align*}\n",
    "H &= \\begin{pmatrix}\\frac{\\partial^2c}{\\partial w_1 \\partial w_1} & \\frac{\\partial^2c}{\\partial w_1 \\partial w_2}\n",
    "\\\\\n",
    "\\frac{\\partial^2c}{\\partial w_2 \\partial w_1} & \\frac{\\partial^2c}{\\partial w_2 \\partial w_2}\\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.2, 1.2])\n",
    "def quadratic_approximation(w):\n",
    "    f, g, H = obj_fn(w0)\n",
    "    dw = w - w0\n",
    "    return f + np.inner(g, dw) + 0.5 * np.inner(dw, np.dot(H, dw)), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "w1 = np.arange(0, 5, 0.1)\n",
    "w2 = np.arange(0, 5, 0.1)\n",
    "contour_plot(obj_fn, w1, w2, levels=np.arange(0.15, 1.8, 0.2), linestyles='dotted')\n",
    "contour_plot(quadratic_approximation, w1, w2, levels=np.arange(0.15, 1.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "Newton's method iterates the following update rule:\n",
    "$$\n",
    "w_{t+1} = w_t - H_t^{-1} \\nabla_f(w_t)\n",
    "$$\n",
    "where $H$ is the Hessian matrix.\n",
    "\n",
    "Let us Newton's method to minimize our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(obj_fn, w1, w2, levels=np.arange(0.15, 1.8, 0.2))\n",
    "\n",
    "w = np.array([1.5, 0.6])\n",
    "lrate = 0.1\n",
    "for i in range(20):\n",
    "    c, g, H = obj_fn(w)\n",
    "    dw = -np.linalg.solve(H, g)\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "    wnew = w + dw\n",
    "    w = wnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input normalization\n",
    "\n",
    "Consider the first toy model we optimized in this notebook. The model is a simple linear neuron with two inputs\n",
    "$$\n",
    "f(x) = w^T x = w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "Our dataset contains two examples:\n",
    "$x_1 = (2, 2)$, $y_1 = 2$,\n",
    "$x_2 = (2, 0)$, $y_2 =0$\n",
    "\n",
    "We estimate the parameters of the model by maximizing the likelihood:\n",
    "$$\n",
    "  w_1, w_2 \\leftarrow \\arg \\min\n",
    "  \\sum_{i=1}^2 \\left(y_i - f(x_i) \\right)^2\n",
    "$$\n",
    "which is equivalent to minimizing\n",
    "$$\n",
    "  c(w) = \\frac{1}{2} w^T A w - b^T w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fn(w, x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      w (array): Array of weights of size (2)\n",
    "    \"\"\"\n",
    "    c = 0.5 * np.square(y - np.dot(w, x)).sum()\n",
    "    g = -np.dot(x, (y - np.dot(w, x)))\n",
    "    H = np.dot(x, x.T)\n",
    "    return c, g, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting the inputs\n",
    "\n",
    "Let us subtract a constant one from the input and visualize the error surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((\n",
    "    np.array([2, 2]),\n",
    "    np.array([2, 0]),\n",
    ")) - 1\n",
    "y = np.array([2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.arange(-1, 3, 0.01)\n",
    "w2 = np.arange(-1, 3, 0.01)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 10, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Hessian of the loss function has two equal eigenvalues which improves the speed of convergence of the gradient descent method. We can see that regardless of the location, the direction of the negative gradient points towards the minimum. In fact, by choosing the optimal learning rate, we can find the minimum in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize negative gradients\n",
    "w1g = np.linspace(-1, 3, 16)\n",
    "w2g = np.linspace(-1, 3, 16)\n",
    "W1g, W2g = np.meshgrid(w1g, w2g)\n",
    "\n",
    "G = np.array([\n",
    "    obj_fn(np.array([_w1, _w2]), x, y)[1]\n",
    "    for _w1, _w2 in zip(W1g.flat, W2g.flat)\n",
    "])  # (N*M, 2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "ax.quiver(W1g.flat, W2g.flat, -2 * G[:, 0], -2 * G[:, 1], scale=50, scale_units='xy', angles='xy')\n",
    "ax.set_xlim([w1[0], w1[-1]])\n",
    "ax.set_ylim([w2[0], w2[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us shift the inputs by adding a constant 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((\n",
    "    np.array([2, 2]),\n",
    "    np.array([2, 0]),\n",
    ")) + 3\n",
    "y = np.array([2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the eigenvalues of the Hessian are substantially different which slows down the convergence of the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.arange(-2.5, 1.5, 0.01)\n",
    "w2 = np.arange(-1, 3, 0.01)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 15, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Let us now scale the inputs by dividing the first dimension by 4 and multiplying the second dimention by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((\n",
    "    np.array([1, 1]),\n",
    "    np.array([1, -1]),\n",
    ")) * np.array([0.25, 4])\n",
    "y = np.array([2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it has negative effect on the optimization landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.arange(0.5, 7.5, 0.02)\n",
    "w2 = np.arange(0.5, 7.5, 0.02)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 15, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.column_stack((\n",
    "    np.array([2, 2]),\n",
    "    np.array([2, 0]),\n",
    "))\n",
    "y = np.array([2, 0])\n",
    "\n",
    "def obj_fn(w, x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      w (array): Array of weights of size (2)\n",
    "    \"\"\"\n",
    "    c = 0.5 * np.square(y - np.dot(w, x)).sum()\n",
    "    g = -np.dot(x, (y - np.dot(w, x)))\n",
    "    H = np.dot(x, x.T)\n",
    "    return c, g, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.arange(-2, 2, 0.01)\n",
    "w2 = np.arange(-1, 3, 0.01)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "contour_plot(lambda w: obj_fn(w, x, y), w1, w2, levels=np.arange(0, 20, 1))\n",
    "\n",
    "w = np.array([0.5, -1])\n",
    "\n",
    "\n",
    "learning_rate = 0.4\n",
    "beta_1 = 0.8\n",
    "beta_2 = 0.8\n",
    "epsilon = 10e-8\n",
    "\n",
    "ms = np.zeros(w.shape)\n",
    "vs = np.zeros(w.shape)\n",
    "\n",
    "for t in range(1, 50+1):\n",
    "    cost, gradient, _ = obj_fn(w, x, y)\n",
    "\n",
    "    # Moving average of first and second order moments\n",
    "    m_iter = beta_1 * ms + (1. - beta_1) * gradient\n",
    "    v_iter = beta_2 * vs + (1. - beta_2) * np.square(gradient)\n",
    "\n",
    "    # Bias corrected first and second order moments (ref paper)\n",
    "    m_iter_corrected = m_iter / (1. - (beta_1 ** t))\n",
    "    v_iter_corrected = v_iter / (1. - (beta_2 ** t))\n",
    "\n",
    "    # Compute the update for the parameter\n",
    "    dw = - learning_rate * m_iter_corrected / (np.sqrt(v_iter_corrected) + epsilon)\n",
    "    ax.quiver(w[0], w[1], dw[0], dw[1], scale=1, scale_units='xy', angles='xy')\n",
    "\n",
    "    # Update parameter\n",
    "    wnew = w + dw\n",
    "    w = wnew\n",
    "    \n",
    "    # Update the values of the first and second order moments\n",
    "    ms = m_iter\n",
    "    vs = v_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
