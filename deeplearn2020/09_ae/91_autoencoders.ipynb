{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09b260f9ae5b77beb841e5a835828f7d",
     "grade": false,
     "grade_id": "cell-e18bc7372632c56c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 1\n",
    "<br>\n",
    "<b>Deadline:</b> May 11, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 9.1. Bottleneck autoencoders\n",
    "\n",
    "The goal of this exercise is to get familiar with bottleneck autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97fbc17672773977ee25387ca826e26b",
     "grade": false,
     "grade_id": "cell-6c689cad698955aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fa17563d39bd63f3e78fbc8bef90470",
     "grade": false,
     "grade_id": "cell-1719dd29ac7b2ba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we use the standard MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68174d3e75d41ad0727674acf1e5d87a",
     "grade": false,
     "grade_id": "cell-3a8ecca2e24ee4be",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Minmax normalization to [-1, 1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ca2663c186e3c9cf2ba9349cebf29d5",
     "grade": false,
     "grade_id": "cell-bc25fbf2c415bdf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.Greys, clim=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3c3ddea5a58dc951fb57f22b2a463f7",
     "grade": false,
     "grade_id": "cell-94867dbc5fc9c8d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Train a deep autoencoder\n",
    "\n",
    "We train a deep autoencoders with only fully-connected layers.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Our encoder will have three hidden layers with ReLU nonlinearities. The exact architecture is not tested. We used the following architecture in our experiments:\n",
    "- a fully-connected layer with 1000 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with 500 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with 250 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with `n_components` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23a7623acb555b0032e270fdbbdba167",
     "grade": false,
     "grade_id": "encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a88aa07dd09b5c78ad8f723395c085a5",
     "grade": false,
     "grade_id": "cell-77976f148263751d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_Encoder_shapes():\n",
    "    n_components = 2\n",
    "    encoder = Encoder(n_components)\n",
    "    \n",
    "    x = torch.randn(3, 1, 28, 28)\n",
    "    y = encoder(x)\n",
    "    assert y.shape == torch.Size([3, n_components]), f\"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e78b5aa9442b6e8157874f6a0956955",
     "grade": false,
     "grade_id": "cell-7984b9a53f198a03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "Our decoder will have three hidden layers with ReLU nonlinearities. The exact architecture is not tested. We used the following architecture in our experiments:\n",
    "- a fully-connected layer with 250 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with 500 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with 1000 units followed by ReLU nonlinearity\n",
    "- a fully-connected layer with 784 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a01df8b87bffc6c9de1b578dbcf080",
     "grade": false,
     "grade_id": "decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02e11c72db0c5accce1d63e3781d6682",
     "grade": false,
     "grade_id": "cell-4c0db481f14d2929",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_Decoder_shapes():\n",
    "    n_components = 2\n",
    "    decoder = Decoder(n_components)\n",
    "    \n",
    "    x = torch.randn(3, n_components)\n",
    "    y = decoder(x)\n",
    "    assert y.shape == torch.Size([3, 1, 28, 28]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6caf996b78ce837c1df46071b98d22ba",
     "grade": false,
     "grade_id": "cell-c0fd59ba80034121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train a bottleneck autoencoder\n",
    "\n",
    "We will use the bottleneck autoencoder to encode MNIST images into 10-dimensional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "009f1d3ec59a8682b2d50bc829a3288e",
     "grade": false,
     "grade_id": "cell-0be3aded6232563a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a deep autoencoder\n",
    "n_components = 10\n",
    "encoder = Encoder(n_components)\n",
    "encoder.to(device)\n",
    "\n",
    "decoder = Decoder(n_components)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90bdf6745da34bfe68e4bd7412dbb0af",
     "grade": false,
     "grade_id": "cell-21e06161069d0c09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. In the training loop, the training data are first encoded into lower-dimensional representations using the encoder. Then, the decoder is used to produce the reconstructions of the original images from the lower-dimensional code. We will use the `MSELoss` to measure the reconstruction error, which is minimized during training.\n",
    "\n",
    "The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.001\n",
    "\n",
    "Hints:\n",
    "- Training usually converges fast, four epochs is usually enough.\n",
    "- The loss at convergence should be close to 0.066."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faf6819518f47043935cf238a18b325a",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "411dac381c30bffd71a7787dc53c64cc",
     "grade": false,
     "grade_id": "cell-a87b586ffde2e123",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '9_ae_encoder.pth')\n",
    "    tools.save_model(decoder, '9_ae_decoder.pth')\n",
    "else:\n",
    "    encoder = Encoder(n_components=10)\n",
    "    tools.load_model(encoder, '9_ae_encoder.pth', device)\n",
    "\n",
    "    decoder = Decoder(n_components=10)\n",
    "    tools.load_model(decoder, '9_ae_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae2c02dec08ad051bcc5ec1f88123f4f",
     "grade": false,
     "grade_id": "cell-33e544e3b0996c9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "Let us visualize the latent space in the cell below. If your autoencoder does a good job, you should clearly see ten clusters corresponding to the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74b4e8a72977fa4ff24710baac65563b",
     "grade": false,
     "grade_id": "cell-88a80116ab8aa4c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tests.visualize_embeddings(encoder, trainloader, n_samples=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f93ef520e663765ea4aa59ff4c28b5",
     "grade": false,
     "grade_id": "cell-ebc5cc2c42ebe7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize test images and their reconstructions using the trained autoencoder\n",
    "tests.visualize_reconstructions(encoder, decoder, trainloader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf2a0e70f0d6c663d0a72db0fc56ab57",
     "grade": false,
     "grade_id": "cell-a2dcd9e8c0a1ae3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test the quality of the produced embeddings by classification\n",
    "\n",
    "We will test the quality of the produced encodings by training a classifier using the encoded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fabd13eb3f68f1d86447a4977000cce",
     "grade": false,
     "grade_id": "cell-c5a8d7261f770312",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "058ed582970c6fc7372694ee23f2ab53",
     "grade": false,
     "grade_id": "cell-41b3c545db653cc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode data samples using the encoder\n",
    "def encode(dataset, encoder):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for images, labels_ in dataloader:\n",
    "            embeddings.append(encoder(images.to(device)))\n",
    "            labels.append(labels_)\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels\n",
    "\n",
    "traincodes, trainlabels = encode(trainset, encoder)  # traincodes is (60000, 10)\n",
    "testcodes, testlabels = encode(testset, encoder)  # testcodes is (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c414fbca302b3fef1197dc14fce1092",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train a simple linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(traincodes.cpu(), trainlabels.cpu())\n",
    "\n",
    "predicted_labels = logreg.predict(testcodes.cpu())  # (10000,)\n",
    "\n",
    "accuracy = np.sum(testlabels.cpu().numpy() == predicted_labels) / predicted_labels.size\n",
    "print('Accuracy with a linear classifier: %.2f%%' % (accuracy*100))\n",
    "assert accuracy > .85, \"Poor accuracy of the embeddings: classification accuracy is %.2f%%\" % (accuracy*100)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b446e18aed39df16e9eee616d6d6a53",
     "grade": false,
     "grade_id": "cell-e43f3ec61532406f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this exercise, we reduced the dimensionality of the MNIST data from $28 \\times 28 = 784$ to $10$ using a bottleneck autoecoder. Using a very simple linear classifier, we were able to classify the encoded images with a good accuracy, which is the evidence that the structure of the data is well preserved in the embedding space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
