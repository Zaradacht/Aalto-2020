{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d9b11112e967a0b426f92979cc97288",
     "grade": false,
     "grade_id": "cell-51463f10ee76cd64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "In this exercise, we will try a few methods to prevent overfitting of an MLP network to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem\n",
    "We will look at a regression problem where the task is to estimate a function of one variable\n",
    "$$y = f(x)$$\n",
    "using a set of training examples $(x_1, y_1), \\ldots, (x_n, y_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baeb8a3430abfe96373c944354ee3f75",
     "grade": false,
     "grade_id": "cell-602ebca68e883d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us first generate training examples $y_i=\\sin(x_i) + n_i$ with $x_i$ drawn from the uniform distribution in $[-0.5, 0.5]$ and noise $n_i$ drawn from the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n = 150\n",
    "x = np.random.rand(n, 1)-0.5\n",
    "\n",
    "def fun(x):\n",
    "    y = np.cos(2* np.pi * x)\n",
    "    y += 0.3 * np.random.randn(*x.shape)\n",
    "    return y\n",
    "\n",
    "y = fun(x)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "083ad76a86a911e9119a37c5e664a35d",
     "grade": false,
     "grade_id": "cell-4ffc8ddcb1221702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us split the data into training, validation and test sets and plot the training and validation sets. And let us plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "torch.manual_seed(3)\n",
    "rp = torch.randperm(x.size(0))\n",
    "\n",
    "n_train = int(x.size(0) * 0.5)\n",
    "x_train, y_train = x[rp[:n_train]], y[rp[:n_train]]\n",
    "x_test, y_test = x[rp[n_train:]], y[rp[n_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f395b4650f0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "fix, ax = plt.subplots(1)\n",
    "ax.plot(x_train, y_train, 'b.')\n",
    "ax.plot(x_test, y_test, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a85c70f5144240f62108369d9e9111c",
     "grade": false,
     "grade_id": "cell-14cf094e013cff8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Define a multi-layer perceptron (MLP) network with two hidden layers\n",
    "\n",
    "In the code below, we define a neural network architecture with:\n",
    "* input dimension 1\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* linear output layer with output dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions to assess the quality of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This visualizes the function implemented by an MLP\n",
    "def plot_fit(mlp, x, y):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x, y, '.')\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(np.linspace(-0.5, 0.5, 100).reshape((-1, 1))).float()\n",
    "        pred = mlp.forward(x)\n",
    "    ax.plot(x, pred)\n",
    "\n",
    "# This is the function to compute the loss:\n",
    "def compute_loss(mlp, x, y):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        return loss.cpu().numpy()\n",
    "\n",
    "# This is the function to print the progress during training\n",
    "def print_progress(epoch, train_error, test_error):\n",
    "    print(f'Epoch {epoch+1}: Train error: {train_error:.2f}, Test error: {test_error:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Plot the function implemented by the MLP\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0556343b1cc1fb1219188a2ccfa636da",
     "grade": false,
     "grade_id": "cell-38e864109561c7c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Train the MLP network without regularization\n",
    "Training is done by minimizing the mean-squared error computed on the training data:\n",
    "$$c=\\sum_{i=1}^n || f(x_i) - y_i ||^2.$$\n",
    "\n",
    "Here, we train the network:\n",
    "* using all the data for computing the gradient (batch mode)\n",
    "* using `n_epochs` epochs (which is equal to the number of parameter updates in the batch mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train error: 0.07, Test error: 0.11\n",
      "Epoch 1000: Train error: 0.07, Test error: 0.11\n",
      "Epoch 1500: Train error: 0.07, Test error: 0.11\n",
      "Epoch 2000: Train error: 0.06, Test error: 0.11\n",
      "Epoch 2500: Train error: 0.06, Test error: 0.13\n",
      "Epoch 3000: Train error: 0.05, Test error: 0.15\n",
      "Epoch 3500: Train error: 0.05, Test error: 0.16\n",
      "Epoch 4000: Train error: 0.05, Test error: 0.19\n",
      "Epoch 4500: Train error: 0.04, Test error: 0.19\n",
      "Epoch 5000: Train error: 0.03, Test error: 0.20\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)\n",
    "plt.plot(x_test, y_test, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a108841c5fbfbb3d4eced5606a0ff05",
     "grade": false,
     "grade_id": "cell-3dbf989a13b25a8a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the network overfits to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_no_regularization = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss without regularization: %.5f\" % test_loss_no_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "851eb7e44885c359e10589f00005f4cf",
     "grade": false,
     "grade_id": "cell-b6cf5359944d980f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Early stopping\n",
    "\n",
    "One of the simplest ways to avoid overfitting is to stop training when the validation error starts to grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3d8061a18ec107f1fcc8c4d7ec0c058",
     "grade": false,
     "grade_id": "cell-3d5bbf165b21b8aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the code below, define a stopping creterion in function `stop_criterion`. Training should be stopped (function returns  `True`) when the validation error is larger than the best validation error obtained so far (with given `tolerance`) for `patience` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          patience (int):    Maximum number of epochs with unsuccessful updates.\n",
    "          tolerance (float): We assume that the update is unsuccessful if the validation error is larger\n",
    "                              than the best validation error so far plus this tolerance.\n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, test_errors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          test_errors (iterable): Validation errors after every update during training.\n",
    "        \n",
    "        Returns: True if training should be stopped: when the validation error is larger than the best\n",
    "                  validation error obtained so far (with given tolearance) for patience epochs (number of consecutive epochs for which the criterion is satisfied).\n",
    "                 \n",
    "                 Otherwise, False.\n",
    "        \"\"\"\n",
    "        if len(test_errors) <= self.patience:\n",
    "            return False\n",
    "\n",
    "        min_val_error = min(test_errors)\n",
    "        test_errors = np.array(test_errors[-self.patience:])\n",
    "        return all(test_errors > min_val_error + self.tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the network with early stopping\n",
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "early_stop = EarlyStopping(tolerance=0.005, patience=20)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "    test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "\n",
    "    if early_stop.stop_criterion(test_errors):\n",
    "        print(test_errors[epoch])\n",
    "        print('Stop after %d epochs' % epoch)\n",
    "        break\n",
    "        \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print_progress(epoch, train_errors[epoch], test_errors[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_early_stopping = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with early stopping: %.5f\" % test_loss_early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d70dd3d3f8e8c2cfb4b55f8c255c03e2",
     "grade": false,
     "grade_id": "cell-898f073dfa615c29",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Weight-decay regularization\n",
    "\n",
    "Let us train the same network with L2 penalties on the weights. In PyTorch, one can add L2 penalty terms for all the parameters by providing `weight_decay` argument for most types of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an MLP with L2 regularization\n",
    "mlp = MLP()\n",
    "\n",
    "# Create an Adam optimizer with learning rate 0.01 and weight decay parameter 0.001\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=0.001)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_weight_decay = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with weight decay: %.5f\" % test_loss_weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34a6e0abf991b63f9628c3317af8959a",
     "grade": false,
     "grade_id": "cell-838adccf05b5b869",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Injecting noise to inputs\n",
    "\n",
    "One way to improve generalization is to add noise to the inputs. So, we update the parameters of $f$ using the gradient of the following function\n",
    "$$c= \\sum_{i=1}^n || f(x_i + n_i) - y_i ||^2$$\n",
    "where $n_i$ is a noise instance. In the code, we corrupt input with Gaussian noise with standard deviation 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP with injecting noise to inputs\n",
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train + 0.07 * torch.randn_like(x_train))\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c6bb460c104299c6ebec89b7beecfd6",
     "grade": false,
     "grade_id": "cell-c8eb739eb3ecb6ec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_inj_noise = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with noise injection: %.5f\" % test_loss_inj_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3f73fc6ac42f0352b08e6c23f44c88d",
     "grade": false,
     "grade_id": "cell-b194dea5c4db3b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Another way to improve generalization is to use dropout. In the cell below, we define an MLP with exactly the same architecture as previously but with `nn.Dropout` layers (with dropout probability 0.2) after each `tanh`\n",
    "nonlinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPDropout, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPDropout()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
    "#scheduler = StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #scheduler.step()\n",
    "    mlp.train()  # Dropout layers work differently during training and test. This sets the training mode.\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        mlp.eval()\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_dropout = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with dropout: %.5f\" % test_loss_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd87d02f28cf3e69ce8cc91e556e575d",
     "grade": false,
     "grade_id": "cell-55bc3b6b8e363d8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reducing model capacity\n",
    "\n",
    "Another simple way to reduce overfitting is to reduce the capacity of the model. Let us use for the same regression task a much smaller network: an MLP with one hidden layer with five units, tanh nonlinearity in the hidden layer and a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPSmall, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(5, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPSmall()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_small = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss by reducing model capacity: %.5f\" % test_loss_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can summarize the results obtained with different regularization methods:\n",
    "print('No regularization: %.5f' % test_loss_no_regularization)\n",
    "print('Early stopping:    %.5f' % test_loss_early_stopping)\n",
    "print('Weight decay:      %.5f' % test_loss_weight_decay)\n",
    "print('Noise injection:   %.5f' % test_loss_inj_noise)\n",
    "print('Dropout:           %.5f' % test_loss_dropout)\n",
    "print('Small network:     %.5f' % test_loss_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01e31500280375eed0bced9cefd41b0b",
     "grade": false,
     "grade_id": "cell-a9fc6f9740d33f87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The values of the hyperparameters (parameters of the training procedure) may have a major impact on the results. The best hyperparameters are usually found by measuring the performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
