{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "662b1c1b203da3d68b8d79cb4aa43aa4",
     "grade": false,
     "grade_id": "cell-e823893319cc2093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 6: Machine Translation Evaluation\n",
    "\n",
    "# Released: 26.03.2020\n",
    "# Deadline: 9.04.2020 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69991079f5710b5e32f0280b956cbd3d",
     "grade": false,
     "grade_id": "cell-2b4616209ec110f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing this assignment, you'll learn how to automatically evaluate MT system using BLEU score.\n",
    "\n",
    "KEYWORDS:\n",
    "* BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ee0af39f57c6bd6e421acfa4e56f28d",
     "grade": false,
     "grade_id": "cell-4f67e9fc3d6c389a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With Machine Translation, it can not only be difficult to find good data to train the system on, but it can also be difficult to evaluate the system's output. The perfect way would be to ask a human to score the system's output. This approach is not really feasible, especially at the stage when you're only tuning the system. Instead of human assessments, we can use some quick automatic approximation of it.\n",
    "\n",
    "## TASK 1\n",
    "## BLEU score\n",
    "\n",
    "**BLEU** aka bilingual evaluation understudy (https://www.aclweb.org/anthology/P02-1040.pdf) is an algorithm for automatic evaluation of translation produced by a MT system. Given a reference translation (or translations) produced by a human, BLEU evaluates how close the machine produced output is to the human translation on a scale from 0 to 1 (the larger the better). \n",
    "\n",
    "BLEU counts N-gram overlaps between machine translation output and its reference translation. These matches are position-independent, the match can happen anywhere in the reference sentence (or sentences). The more matches, the better the candidate translation is. We also want to punish the system for giving out a translation that only has a couple of words (think why, it will be a question in the last task).\n",
    "The score is calculated in the following way:\n",
    "\n",
    "$BLEU = min (1, \\frac{output-length}{reference-length})(\\prod_{i=1}^{N} precision_{i})^{\\frac{1}{N}}$\n",
    "\n",
    "As you probably remember from IR assignment, precision measures how many right answers were given out of all answers. It is calculated in the following way:\n",
    "Precision = $\\frac{tp}{tp + fp}$\n",
    "\n",
    "So for the N-gram case, where N is 4, the computation of the BLEU score will be:\n",
    "* STEP 1: count unigram precision: how many words out of translation were seen in the references.\n",
    "* STEP 2: count 2-grams precision: how many bi-grams out of all bi-grams in translation were seen in the bi-grams fron the references.\n",
    "* STEP 3: count 3-gram precision\n",
    "* STEP 4: count 4-gram precision\n",
    "* STEP 5: count average precision\n",
    "* STEP 6: apply brevity penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8fb803d7571987ad4e548385a28862d",
     "grade": false,
     "grade_id": "cell-5a5cd9018823435c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Count n-gram matches\n",
    "## 1.1\n",
    "Write a function that will estimate the n-gram precision. Once an n-gram is matched to something, a reference n-gram should be considered exhausted (you can't match anything new to it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9d0296dff789e9acc948bc4eaa80317",
     "grade": false,
     "grade_id": "cell-bf26f9a957a91606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_ngram_matches(translated_sentence, reference_sentences, n):\n",
    "    \"\"\"\n",
    "    this function takes in a translated sentence, a list of reference sentences\n",
    "    and a length of ngrams we want to look for,\n",
    "    and outputs the proportion of ngrams from the translated sentence that were found in the references.\n",
    "    \n",
    "    INPUT:\n",
    "    translated_sentence - a sentence that is being compared to a reference (list of strings)\n",
    "    reference_sentences - a list of reference sentences (list of lists of strings)\n",
    "    n - the length of ngrams that are compared (integer)\n",
    "    OUTPUT:\n",
    "    precision - the fractions of matched ngrams out of all ngrams in the sentence1 (float)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tp = 0\n",
    "    for sentence in reference_sentences:\n",
    "        sen = ' '.join(sentence)\n",
    "        for i in range(len(translated_sentence) - n + 1):\n",
    "            if ' '.join(translated_sentence[i:i + n]) in sen:\n",
    "                tp += 1\n",
    "    precision = tp / (len(translated_sentence) - n + 1)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9496391c9ba7f56abf1ff5ba7524e28",
     "grade": true,
     "grade_id": "cell-3518b9b70865dff5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_translated_sentence = ['a','b','c','d','3']\n",
    "dummy_ref_sentence = [['a','b','c','4']]\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a float number\n",
    "assert_equal(type(count_ngram_matches(dummy_translated_sentence, dummy_ref_sentence, 2)), float)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_almost_equal(count_ngram_matches(dummy_translated_sentence, dummy_ref_sentence, 1), 0.6, 2)\n",
    "\n",
    "assert_almost_equal(count_ngram_matches(dummy_translated_sentence, dummy_ref_sentence, 2), 0.5, 2)\n",
    "\n",
    "assert_almost_equal(count_ngram_matches(dummy_translated_sentence, dummy_ref_sentence, 3), 0.33, 2)\n",
    "\n",
    "\n",
    "dummy_translated_sentence2 = ['1','2','1']\n",
    "dummy_ref_sentence2 = [['1','1','3'],['4','3'],['3']]\n",
    "\n",
    "assert_almost_equal(count_ngram_matches(dummy_translated_sentence2, dummy_ref_sentence2, 1), 0.67 ,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e286a89068878877625fe02f80d8cf5d",
     "grade": false,
     "grade_id": "cell-68c5e02f9fade5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Estimate a brevity penalty\n",
    "## 1.2\n",
    "Now let's punish our translation for being too short. Compare the length of the translated sentence with the length of the longest of the references. If tranlation is longer than any reference, your function should output 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38e927622f782ad5fcca29d500b61a4f",
     "grade": false,
     "grade_id": "cell-137382ee46d4f47f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def brevity_penalty(len_translation, len_references):\n",
    "    \"\"\"\n",
    "    this function takes in the length of the translated snetence and a list of lengths of the reference sentences,\n",
    "    and outputs a brevity penalty compared to the longest out of reference sentences\n",
    "    \n",
    "    INPUT:\n",
    "    len_translation - the number of words in the translated sentence (integer)\n",
    "    len_references - a list of reference sentences lengths [1,3,5] (a list of integers)\n",
    "    OUTPUT:\n",
    "    score - brevity penalty score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    frac = [len_translation / l for l in len_references]\n",
    "    score = min(1, min(frac))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7c6af9aeafc343cf77b6dc9000a3f01",
     "grade": true,
     "grade_id": "cell-1e6d9410ea215013",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_len_translation = 11\n",
    "dummy_len_references = [12, 9, 11]\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_almost_equal(brevity_penalty(dummy_len_translation, dummy_len_references), 0.92, 2)\n",
    "\n",
    "dummy_len_translation2 = 15\n",
    "dummy_len_references2 = [12, 9, 11]\n",
    "\n",
    "assert_equal(brevity_penalty(dummy_len_translation2, dummy_len_references2), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d88c9a77e2992f9ac60c8bfba2796d7b",
     "grade": false,
     "grade_id": "cell-771b3a9511d24d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Combine all elements\n",
    "## 1.3\n",
    "Now let's compose everything into one function that outputs BLEU score. It should take a translated sentence, a list of references and the maximun length of n-grams and output a BLEU score.\n",
    "\n",
    "There can be a situation, when there are no n-grams seen in the reference sentences. If that happens to the unigram translation, the score should be 0. In other cases, instead of givin 0 for precision, smoothe it to be $\\frac{1}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "260f2e7369470f54d9c536b55634f451",
     "grade": false,
     "grade_id": "cell-699f52568aa9243a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def BLEU(translation, references, n):\n",
    "    \"\"\"\n",
    "    this function takes in a translation sentence\n",
    "    \n",
    "    INPUT:\n",
    "    translation - a translated sentence (a list of strings)\n",
    "    references - a list of reference sentences (a list of lists of strings)\n",
    "    OUTPUT:\n",
    "    score - a BLEU score for the translation\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ref_len = [len(r) for r in references]\n",
    "    penalty = brevity_penalty(len(translation), ref_len)\n",
    "    precision = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        i = i + 1\n",
    "        p = count_ngram_matches(translation, references, i)\n",
    "        if p == 0:\n",
    "            if precision == 0:\n",
    "                continue\n",
    "            else:\n",
    "                precision *= (1 / n)\n",
    "        else:\n",
    "            if precision == 0:\n",
    "                precision = p\n",
    "            else:\n",
    "                precision *= p\n",
    "    precision = precision ** (1 / n)\n",
    "    score = precision * penalty\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdde792e20760eae73d59c5769c6b436",
     "grade": true,
     "grade_id": "cell-6f99838ac4aa71cc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_translated_sentence = ['1','2','3','4']\n",
    "dummy_ref_sentence = [['1','2','3','5']]\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a float number\n",
    "assert_equal(type(BLEU(dummy_translated_sentence, dummy_ref_sentence, 2)), float)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_almost_equal(BLEU(dummy_translated_sentence, dummy_ref_sentence, 4), 0.5, 3)\n",
    "\n",
    "dummy_translated_sentence2 = ['1']\n",
    "dummy_ref_sentence2 = [['1','2','3','5']]\n",
    "\n",
    "assert_almost_equal(BLEU(dummy_translated_sentence2, dummy_ref_sentence2, 1), 0.25, 3)\n",
    "\n",
    "dummy_translated_sentence3 = ['1','2']\n",
    "dummy_ref_sentence3 = [['1','3','1'],['2']]\n",
    "\n",
    "assert_almost_equal(BLEU(dummy_translated_sentence3, dummy_ref_sentence3, 2), 0.471, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6696aa2d33e3ba73d94e1942f166580a",
     "grade": false,
     "grade_id": "cell-df2e90f797befcaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Your reflection on BLEU score\n",
    "## 1.4\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Will providing more references increase the BLEU score? Why?\n",
    "\n",
    "2. What is the benefit of brevity score? Why do we need it?\n",
    "\n",
    "3. Will a human translator always get a BLEU score of 1? Why?\n",
    "\n",
    "4. What are the problems with the way BLEU calculates its score? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "324538de940f16008e5f853338854de3",
     "grade": true,
     "grade_id": "cell-c038d749e95ea6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
