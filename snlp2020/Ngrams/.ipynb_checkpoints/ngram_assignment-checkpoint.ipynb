{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f12428dc06a9630f22a5cccd65928b76",
     "grade": false,
     "grade_id": "cell-06d639cce7633bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 3: N-gram language models\n",
    "\n",
    "# Released: 22.1.2019\n",
    "# Deadline: 5.2.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f03d4ad359534798b03bc53cb0dc486",
     "grade": false,
     "grade_id": "cell-3bfa793aa2cd6b17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing this assignment, you'll understand how statical language models can be estimated. You'll be able to evaluate them and to generate text using them.\n",
    "\n",
    "KEYWORDS:\n",
    "\n",
    "- Language models\n",
    "- N-grams\n",
    "- Perplexity\n",
    "- Additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ae5245747ed32f1e9ef72597a7a4775",
     "grade": false,
     "grade_id": "cell-eb34778ec1241a7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data\n",
    "Jane Austen's Pride and Prejudice, in `/coursedata/pride-and-prejudice.txt`\n",
    "\n",
    "### Libraries\n",
    "In this task you'll use the following libraries:\n",
    "- [random](https://docs.python.org/3/library/random.html) -  is a module that implements pseudo-random number generators for various distributions.\n",
    "- [maths](https://docs.python.org/3/library/math.html) - is a module providing access to the mathematical functions defined by the C standard.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "167924bc3c21e6dcddd923983809e3b7",
     "grade": false,
     "grade_id": "cell-ecfa9a14c6632972",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1\n",
    "## Warm up: estimate probability scores by hand\n",
    "As you've already noticed in the first assignment, different language sequences are not equally likely to occur. We've only looked at word frequencies and at frequencies of letter sequences, but what if we want to estimate how probable it is to see some sentence? Well, for this purpose you'll need a **language model**.\n",
    "\n",
    "A **language model** predicts the following word (or other symbol) given the observed history.\n",
    "$P(w_i| w_{i−1} . . . w_0)$.\n",
    "**Language models** are useful, for example, in the task of speech recognition. They help to distinguish between homophones (words that sound the same but have different meanings), for example, _\"to\"_ and _\"too\"_ in _\"I love you too\"_ and _\"I love you to death\"._\n",
    "\n",
    "### What is an n-gram language model?\n",
    "An **n-gram language model** approximates the probability of a word given all the previous words by using only the conditional probability of the $N-1$ preceding words. This approach is based on the Markov assumption: the next word depends only on a fixed-size window of previous words and not on the whole history: $P(w_i| w_{i−1} . . . w_{i-h})$\n",
    "<img src= \"../../../coursedata/notebook_illustrations/n_gram.png\">\n",
    "To use an n-gram language model, we need estimates of the probability of seeing a particular word given the recent\n",
    "history. For instance, in the 3-gram model, the history is 2 words.\n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of history\n",
    "* 1 word for **prediction**\n",
    "\n",
    "\n",
    "An intuitive way to estimate probabilities is calculating **maximum likelihood estimation** (MLE). To get the MLE estimate of an n-gram model we get counts from a corpus and normalize these counts to lie between 0 and 1. In the case of bigram language model, when we want to get a probability of some particular bigram $P(w_n|w_{n−1})$, we’ll compute the count of the bigram $C(w_{n−1}w_n)$ and normalize it by the sum of all the bigrams that start with the same first word $w_{n−1}$. It is easy to notice that the sum of all bigram counts that have the same fist word is simply the unigram count for that word $w_{n−1}$. Thus, we get:\n",
    "\n",
    "$P(w_n|w_{n−1}) = \\frac{C(w_{n−1}w_n)}{C(w_{n−1})}$, where\n",
    "$C$ tells the number of occurrences in the training\n",
    "set.\n",
    "\n",
    "The probability of the entire word sequence can be computed using **the chain rule of probability**. And considering the bigram assumption, it is:\n",
    "\n",
    "$P(w_1^n) ≈ \\prod_{k=1}^{n}P(w_k|w_1^{k−1}) ≈ \\prod_{k=1}^{n}P(w_k|w_{k−1}) $\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/n_gram_chain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bae1d7587cae8c2bf7e3164142849876",
     "grade": false,
     "grade_id": "cell-0d9a1dfc564fd1c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compute MLE estimates for a bigram model by hand\n",
    "## 1.1\n",
    "\n",
    "Everything is easy peasy as a concept, but there are some things to consider in practice. We need to augment our corpus by giving each sentence a special start-symbol **&lt;s>**, and a special end-symbol **&lt;/s>**. The start-symbol provides the context for the first word, so we know the words that are more likely to begin a sentence. The end-symbol makes it possible for the n-gram grammar to be a true probability distribution. Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one. \n",
    "\n",
    "Given the corpus below with all its start and end symbols,\n",
    "\n",
    "1. \"&lt;s> say hello to my little friend &lt;/s>\"\n",
    "2. \"&lt;s> say hello &lt;/s>\"\n",
    "3. \"&lt;s> say it to my hand &lt;/s>\"\n",
    "\n",
    "etimate probabilities of the following bigrams:\n",
    "* P(hello|say)\n",
    "* P(my|to)\n",
    "* P(to|hello)\n",
    "* P(say|&lt;s>)\n",
    "* P(my|say)\n",
    "\n",
    "Write your estimates as values into the variables in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bc4a77d4920943dc697862cf11bd516",
     "grade": false,
     "grade_id": "cell-d02559de98ee6bb1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p_say_hello = 0.6666667 # type in the answer as a number between 0 and 1. For example:\n",
    "# p_say_hello = 1.\n",
    "p_to_my = 1 # type in the answer as a number between 0 and 1. For example:\n",
    "# p_to_my = 1.\n",
    "p_hello_to = 0.5 # type in the answer a number between 0 and 1. For example:\n",
    "# p_hello_to = 1.\n",
    "p_start_say = 1 # type in the answer a number between 0 and 1. For example:\n",
    "# p_start_say = 1.\n",
    "p_say_my = 0 # type in the answer a number between 0 and 1. For example:\n",
    "# p_say_my = 1.\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26341e484ab8129b9afaf60d0cbaba21",
     "grade": true,
     "grade_id": "cell-9f0a3220e178b273",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60be2b867bde4451e33d825eac3c1e65",
     "grade": false,
     "grade_id": "cell-8f444a72729f4d30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2\n",
    "## Building an n-gram model\n",
    "### 2.1 pad sentences with start and end symbols\n",
    "\n",
    "As we already mentioned, when dealing with language, it is very important to know what words tend to start and end sentences. To learn this with n-grams, we create special symbols of sentence beginning _\"&lt;s>\"_ and sentence end _\"&lt;/s>\"_. As a pre-processing step, you need to \"pad\" your sentences with these symbols and then create n-grams.\n",
    "\n",
    "Write a function that takes in a list of tokens, puts the needed amount of special start and end symbols in it so that we would definitely know what token or token sequence starts and ends this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68ef5e3806ed6554378feafe34658862",
     "grade": false,
     "grade_id": "cell-98a624d48aba3931",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pad(token_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokens and pads them with special symbols\n",
    "    \n",
    "    INPUT:\n",
    "    token_list - a list of tokens to be padded\n",
    "    n - the length of a token sequence\n",
    "    OUTPUT:\n",
    "    padded_list - a padded list of tokens\n",
    "    \"\"\"\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    # YOUR CODE HERE\n",
    "    # use append to add to the tail of the list\n",
    "    # use insert(0, ..) to add to the head of the list\n",
    "    padded_list = []\n",
    "    for x in range(0, n-1):\n",
    "        padded_list.insert(0, start)\n",
    "    padded_list = padded_list + token_list\n",
    "    for x in range(0, n-1):\n",
    "        padded_list.append(end)\n",
    "    #raise NotImplementedError()\n",
    "    print(\"============ list ============\")\n",
    "    print(padded_list)\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e9191cd890aaf0d276ddd12d6fa0870",
     "grade": true,
     "grade_id": "cell-86c59d8df560ca01",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ list ============\n",
      "['<s>', 'a', 'b', 'c', '</s>']\n",
      "============ list ============\n",
      "['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(pad(['a','b','c'],2), ['<s>', 'a', 'b', 'c', '</s>'])\n",
    "assert_equal(pad(['a','b','c'],3), ['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abbe872ba14304b0fdd0f43c2b12d776",
     "grade": false,
     "grade_id": "cell-a0f997ab8969c491",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 convert a sentence into n-grams\n",
    "\n",
    "Let's go on and create a function that takes a list of tokens and creates an array of n-grams. The sequence of tokens is very important, so make sure your n-grams are immutable (tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7de4a375f215d9d2733c1186c118302c",
     "grade": false,
     "grade_id": "cell-81b21c671cf892a4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    " def make_n_grams(token_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokens and forms a list of n-grams (tuples)\n",
    "\n",
    "    INPUT:\n",
    "    token_list - a list of tokens to be converted into n-grams\n",
    "    n - the length of a token sequence in an n-gram\n",
    "    OUTPUT:\n",
    "    n_grams - a list of n-gram tuples\n",
    "    \"\"\"\n",
    "    if n > len(token_list):\n",
    "        print(\"The N is too large.\")\n",
    "    # YOUR CODE HERE\n",
    "    cur_index = 0\n",
    "    n_grams = []\n",
    "    while cur_index + n <= len(token_list):\n",
    "        n_grams.append(tuple(token_list[cur_index: cur_index + n]))\n",
    "        cur_index += 1\n",
    "        print(\"========== list ============ \")\n",
    "        print(n_grams)\n",
    "    # raise NotImplementedError()\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a0d6ebd3f7cd3b3c19ac02d38660ea6",
     "grade": true,
     "grade_id": "cell-c249921375a3c149",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== list ============ \n",
      "[('a', 'b')]\n",
      "========== list ============ \n",
      "[('a', 'b'), ('b', 'c')]\n",
      "========== list ============ \n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
      "========== list ============ \n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')]\n",
      "========== list ============ \n",
      "[('a', 'b', 'c')]\n",
      "========== list ============ \n",
      "[('a', 'b', 'c'), ('b', 'c', 'd')]\n",
      "========== list ============ \n",
      "[('a', 'b', 'c'), ('b', 'c', 'd'), ('c', 'd', 'e')]\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(make_n_grams(['a','b','c','d','e'], 2), [('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')])\n",
    "assert_equal(make_n_grams(['a','b','c','d','e'], 3), [('a', 'b', 'c'), ('b', 'c', 'd'), ('c', 'd', 'e')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04f1c44dbcc02ddd96d8ff20c2cd3794",
     "grade": false,
     "grade_id": "cell-f6809f85a87bc520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 get n-gram counts\n",
    "Now we can collect statistics from our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbfcf601aac567caacb11cad3ea6dbb8",
     "grade": false,
     "grade_id": "cell-83cb415fd86ec1b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "\n",
    "def get_counts(sentence_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokenized and padded sentences,\n",
    "    forms a list of n-grams and gives out a dictionary \n",
    "    with counts for every seen n-gram\n",
    "    \n",
    "    INPUT:\n",
    "    sentence_list - a list of tokenized and padded sentences to be converted into n-grams\n",
    "    n - the length of a token sequence\n",
    "    OUTPUT:\n",
    "    n_gram_dict - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    {('a',): {'b': 3 'c': 4}\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_list = make_n_grams(sentence_list, n)\n",
    "    groups = []\n",
    "    for i in tokenized_list:\n",
    "        for k, g in groupby(i, lambda k: (k[0:n - 1])):\n",
    "            groups.append(tuple((k, list(g)[0][n - 1])))\n",
    "        \n",
    "    default_dict = defaultdict(list)\n",
    "\n",
    "    for i, j in groups:\n",
    "        default_dict[tuple(i)].append(j)\n",
    "        \n",
    "    n_gram_dict = dict((i, dict(Counter(j))) for i, j in default_dict.items())\n",
    "        \n",
    "    return n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04852846c239c767528ef950c3f09cf6",
     "grade": true,
     "grade_id": "cell-a2b258487651966d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ list ============\n",
      "['<s>', '<s>', 'say', 'hello', 'to', 'my', 'little', 'friend', '</s>', '</s>']\n",
      "============ list ============\n",
      "['<s>', '<s>', 'say', 'hello', '</s>', '</s>']\n",
      "============ list ============\n",
      "['<s>', '<s>', 'say', 'it', 'to', 'my', 'hand', '</s>', '</s>']\n",
      "========== list ============ \n",
      "[(['<s>', '<s>', 'say', 'hello', 'to', 'my', 'little', 'friend', '</s>', '</s>'], ['<s>', '<s>', 'say', 'hello', '</s>', '</s>'], ['<s>', '<s>', 'say', 'it', 'to', 'my', 'hand', '</s>', '</s>'])]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "{'say': 1} != {'say': 3}\n- {'say': 1}\n?         ^\n\n+ {'say': 3}\n?         ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8c2204d50fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdummy_model_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_corpus_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_model_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<s>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<s>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'say'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36massertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m    838\u001b[0m         \u001b[0massertion_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getAssertEqualityFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0massertion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertNotEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36massertDictEqual\u001b[0;34m(self, d1, d2, msg)\u001b[0m\n\u001b[1;32m   1136\u001b[0m                            pprint.pformat(d2).splitlines())))\n\u001b[1;32m   1137\u001b[0m             \u001b[0mstandardMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncateMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardMsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertDictContainsSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36mfail\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;34m\"\"\"Fail immediately, with the given message.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertFalse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: {'say': 1} != {'say': 3}\n- {'say': 1}\n?         ^\n\n+ {'say': 3}\n?         ^\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "assert_equal(dummy_model_3[('<s>', '<s>')], {'say': 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe5b2bfd3128f1438980697a2c6619d6",
     "grade": false,
     "grade_id": "cell-8f605b50c75c642c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.4 compute an MLE language model\n",
    "\n",
    "We already have all the tools to form the items to count in a corpus, now we need to just count them.\n",
    "We've already looked at the **Maximum Likelihood Estimate** for bigrams. In general case it works like this:\n",
    "\n",
    "$P(w_n|w^{n−1}_{n−N+1}) = \\frac{C(w^{n−1}_{n−N+1}w_n)}{C(w^{n−1}_{n−N+1})}$, where $C$ tells the number of occurrences in the training corpus. Basically, you just divide the observed frequency of a particular n-gram by the observed frequency of its \"history\" part. \n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/mle.png\">\n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of history\n",
    "* 1 word for **prediction**\n",
    "\n",
    "Here, the whole 3-gram is (say hello to), and its history part is (say hello). If some n-gram is absent from the training corpus, its MLE estimate would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef26f6c59a842712a2e20ce62fcb7926",
     "grade": false,
     "grade_id": "cell-d7db46c1ece8df57",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def score_mle(model_counts, n_gram, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a dictionary of ngram counts and some n-gram,\n",
    "    and gives out an MLE estimate for this n-gram\n",
    "    \n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "        {('a',): {'b': 3 'c': 4}\n",
    "    n_gram - an ngram as tuple\n",
    "    scoring_parameters - additional, optional scoring parameters, making the function interface generic, \n",
    "        however not used here.\n",
    "    \n",
    "    OUTPUT:\n",
    "    mle_score - MLE score for the n_gram\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(list(model_counts.keys())[0]) + 1\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return ngram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c04730d7c936068c9a65ef17eec6e39",
     "grade": true,
     "grade_id": "cell-10fc716c6bb7dbfb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"<s>\",\"say\", \"it\")),0.33, 2)\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"say\",\"it\", \"friend\")), 0, 2)\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"say\",\"wow\", \"now\")), 0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a8bcd77a01543884e6b707a3fb65c70",
     "grade": false,
     "grade_id": "cell-eaee922237714ef5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.5  Smoothing counts\n",
    "Simple N-gram models have one very serious limitation: they are unable to give a probability estimate not only for n-grams with new out-of-vocabulary words but also for the n-grams with known vocabulary but unseen during training. The higher the $N$, the sparser the data, and the more zero counts there will be. To overcome this problem, we need to redistribute some probability mass from more frequent events and give it to the events we’ve never seen. These techniques are called **smoothing** or **discounting**. \n",
    "\n",
    "In this assignment, you will implement **Additive Smoothing**. In practice, it's not the most successful smoothing method, but it will give you an understanding of the intuition behind even more elaborate techniques.\n",
    "\n",
    "The idea of this method is that we pretend we’ve seen each n-gram $\\delta$ times more than we actually have. Typically, $0 < \\delta ≤ 1$. This way, we just add $\\delta$ to the original counts.\n",
    "\n",
    "$P(w_i|w^{i-1}_{i-n+1}) = \\frac{\\delta + C(w_{i-n+1}^i)}{\\delta|V|+C(w^{i-1}_{i−n+1})}$\n",
    "\n",
    "The $|V|$ is the (prediction) vocabulary of all possible continuations for an n-gram. It can be any word from a sentence, a special end symbol, but not a special start symbol (because it appears only as an (n-1)-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4af82634ea51ebd07e906b08fe217f77",
     "grade": false,
     "grade_id": "cell-d2c08bd2c805779f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def score_smoothed(model_counts, n_gram, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a dictionary of ngram counts, some ngram and delta to be added to this ngram's score,\n",
    "    and gives out a smoothed estimate for this ngram.\n",
    "    if some word in an n-gram is unseen during training, the estimate is zero.\n",
    "    \n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "        where their values are a dictionary of all continuations and their counts\n",
    "        {('a',): {'b': 3 'c': 4}\n",
    "    n_gram - an ngram as a tuple\n",
    "    scoring_parameters - additional, optional scoring parameters, which make the function interface generic\n",
    "        here we will look for scoring_parameters[\"delta\"] - the delta value to be added to the counts\n",
    "                        and for scoring_parameters[\"vocab\"] -  the vocabulary that can be used as\n",
    "                        a continuation of an (n-1)-gram.\n",
    "    \n",
    "    OUTPUT:\n",
    "    smoothed_score - a smoothed score for the n_gram\n",
    "    \"\"\"\n",
    "    delta = scoring_parameters[\"delta\"]\n",
    "    vocab = scoring_parameters[\"vocab\"]\n",
    "    context_length = len(next(iter(model_counts))) \n",
    "    n = context_length + 1 # the ngram length n\n",
    "    context = n_gram[:context_length]\n",
    "    word_to_predict = n_gram[-1]\n",
    "    if any(token not in vocab | {'<s>'} for token in n_gram):\n",
    "        raise ValueError(\"Distribution not defined on this n_gram:\" + repr(n_gram))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return smoothed_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a25bbcb6178a0781738ab54a4e23a61f",
     "grade": true,
     "grade_id": "cell-aec5c000ef8b5356",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "            \n",
    "assert_almost_equal(score_smoothed(dummy_model_3, \n",
    "                                   (\"<s>\",\"say\", \"it\"), \n",
    "                                   delta=1, \n",
    "                                   vocab=dummy_model_vocab), 0.16, 2)\n",
    "\n",
    "assert_almost_equal(score_smoothed(dummy_model_3, \n",
    "                                   (\"say\",\"it\", \"friend\"), \n",
    "                                   delta=1, \n",
    "                                   vocab=dummy_model_vocab), 0.1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a68359f7a442dc1bf15fcf963529824",
     "grade": false,
     "grade_id": "cell-14ebdff021b00d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3\n",
    "## How to evaluate a language model?\n",
    "\n",
    "The best way to evaluate a language model is to look at its performance in the intended application. Unfortunately, it is usually time-consuming to run the whole system just to test the language model parameters. Instead, we can measure how well an n-gram model predicts unseen data called the test set or test corpus. The higher the probability that the model assigns to the test set, the better this model performs. \n",
    "\n",
    "### 3.1 probability of a sentence\n",
    "Let's compute the probability of a test sentence. Remember we were talking about **the chain rule of probability**? This is exactly the right moment to use it.\n",
    "\n",
    "The language model probabilities are usually represented and computed in log format (as log probabilities). It has one very simple explanation: probabilities are small numbers between 0 and 1, and when we multiply these probabilities, we get even smaller product. We take a log of probabilities just to avoid numerical underflow.\n",
    "\n",
    "Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61c4e1444c7ae8fbfaa739e22ff7f878",
     "grade": false,
     "grade_id": "cell-3639f7a6c9ff750b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sentence_logprob(sentence, model_counts, score_function, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a tokenized sentence, language model counts, and a score function.\n",
    "    it pads the sentence with special symbols,\n",
    "    and gives out its probability according to the n-gram model and the scoring method\n",
    "    \n",
    "    INPUT:\n",
    "    sentence - a tokenized sentence\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    score_function - a function which takes a dictionary of counts, an n-gram, and possible additional parameters,\n",
    "        and produces a score for the last token of the n-gram, given the rest as context\n",
    "    scoring_parameters - additional, optional scoring parameters, passed to score_function\n",
    "        like this: score_function(model_counts, ngram, **scoring_parameters)\n",
    "    OUTPUT:\n",
    "    logprob - a log probability score of a sentence\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts))) \n",
    "    n = context_length + 1 # the ngram length n\n",
    "    sentence_grams = make_n_grams(pad(sentence, n), n)\n",
    "    logprob = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21a7c4313dd71c9d5fce5a9932c09b9",
     "grade": true,
     "grade_id": "cell-78aa138f8ce0c644",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,2) for sentence in dummy_corpus]\n",
    "dummy_model_2 = get_counts(dummy_corpus_padded, 2)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "dummy_test_sentence1 = [\"say\", \"hello\", \"to\", \"my\", \"hand\"]\n",
    "dummy_test_sentence2 = [\"say\", \"hello\", \"to\", \"my\", \"little\", \"hand\"]\n",
    "\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence1, dummy_model_2, score_mle), -1.791, 2)\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence2, dummy_model_2, score_mle), -float('inf'), 2)\n",
    "\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence1, dummy_model_2, score_smoothed, delta=1, vocab=dummy_model_vocab), -8.803,2)\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence2, dummy_model_2, score_smoothed, delta=1, vocab=dummy_model_vocab), -11.106,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "836f8c8d8f394913f48be8838e4006f3",
     "grade": false,
     "grade_id": "cell-6198ad0efc830a83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In practice, the raw probability is not used as a metric for evaluating language models. \n",
    "We can look at **perplexity** and **out-of-vocabulary rate** (OOV rate) instead. \n",
    "### 3.2 perplexity by hand\n",
    "\n",
    "**Perplexity** score (PP) tells us how uncertain the model is when predicting the words in the test data $W$. You can think of it as the weighted average branching factor of a language. The branching factor of a language is the number of possible next words that can follow any word. Technically, the perplexity of a language model on some test set is the inverse probability of the test set, normalized by the number of tokens in it.\n",
    "\n",
    "$PP(W) = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1...w_{i-1})}}$\n",
    " \n",
    "In our case, let's think of $N$ as of a number of n-grams the test set contains.\n",
    "\n",
    "Let's look at the dummy language where you only have 4 letters (a, b, c, d). These letters in all possible texts appear with equal probability $P = \\frac{1}{4}$. You have a 5 letter sentence composed in this language. What is the perplexity of this dummy language as a unigram model? \n",
    "\n",
    "Write your answer as a value into the variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4eba266147f607808f451a8b566b129",
     "grade": false,
     "grade_id": "cell-ac36a8d340e2e90b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_ppl = None # type in the answer, and remove the raise NotImplementedError line.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20145cbb99ebf199c1fd3a478be423d6",
     "grade": true,
     "grade_id": "cell-da11fb53720d74b7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4838eca9ddf07c6df37912e2fecfd217",
     "grade": false,
     "grade_id": "cell-a038b27aa5b1d7d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4 perplexity as a function\n",
    "Write a function for calcualting the perplexity of a test corpus. When the model encounters an unseen n-gram, its perplexity should be infinity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78cd92404613a53e73a9ed9f594eea34",
     "grade": false,
     "grade_id": "cell-3d20eab8204b8cfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(text, model_counts, score_function, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in test text and the n-gram model counts and gives out the perplexity of this n-gram model\n",
    "    \n",
    "    INPUT:\n",
    "    text - a list of lists of tokenized sentences\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    {('a',): {'b': 3 'c': 4}\n",
    "    score_function - a function which takes a dictionary of counts, an n-gram, and possible additional parameters,\n",
    "        and produces a score for the last token of the n-gram, given the rest as context\n",
    "    scoring_parameters - additional, optional scoring parameters, passed to score_function\n",
    "        like this: score_function(model_counts, ngram, **scoring_parameters)\n",
    "\n",
    "    OUTPUT:\n",
    "    ppl - a perplexity score of a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    context_length = len(next(iter(model_counts))) \n",
    "    n = context_length + 1 # the ngram length n\n",
    "    logprob = 0\n",
    "    num_predictions = 0 # NOTE: The number of predictions per sentence is len(sentence) + context_length\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d251cd9799a5629ba7a1001e0ed744",
     "grade": true,
     "grade_id": "cell-214c2796f178e3c2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "dummy_test_corpus1 = [[\"say\", \"hello\", \"to\", \"my\", \"hand\"]]\n",
    "dummy_test_corpus2 = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"hand\"]]\n",
    "\n",
    "assert_almost_equal(perplexity(dummy_test_corpus1, dummy_model_3, score_mle), 1.2917, 2)\n",
    "assert_equal(perplexity(dummy_test_corpus2, dummy_model_3, score_mle), float('inf'))\n",
    "\n",
    "assert_almost_equal(perplexity(dummy_test_corpus1, \n",
    "                               dummy_model_3, \n",
    "                               score_smoothed, \n",
    "                               delta=1,\n",
    "                               vocab=dummy_model_vocab), 4.6266, 2)\n",
    "assert_almost_equal(perplexity(dummy_test_corpus2, \n",
    "                        dummy_model_3, \n",
    "                        score_smoothed, \n",
    "                        delta=1,\n",
    "                        vocab=dummy_model_vocab), 5.4829, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b4ad1ccc93889863e79ab9c89b5727b",
     "grade": false,
     "grade_id": "cell-05d07876b4ed295e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5 \n",
    "## Generate text\n",
    "### 5.1 visualize the model\n",
    "Another useful way to make a sanity check of how model is performing, is by generating sentences with it.\n",
    "\n",
    "1. Start with the appropriate number (n-1) of start symbols _\"&lt;s>\"_, e.g. (\"&lt;s>\",\"&lt;s>\") for trigrams.\n",
    "2. Generate tokens one at a time. Given the context, you get a probability distribution over the next word. Sample from that distribution. NOTE: For sampling, use random.choices(), calling it exactly once per token.\n",
    "3. Stop generating when the end symbol is produced.\n",
    "\n",
    "HINT: Use random.choices()\n",
    "\n",
    "NOTE: Although it should be possible to make the pseudorandom behaviour reproducible by setting a seed,\n",
    "    we were not satisfied with the robustness of that solution. Therefore, the generation functions are not\n",
    "    autograded. However, they are used later when working with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d3560a7fc2e8e3190cef203f95f9060",
     "grade": false,
     "grade_id": "cell-2ce1b6c8fecee7fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1.1 MLE text generation\n",
    "First we will implement text generation for the MLE case. In this case, you can simply use the true counts of each n-gram as the weights for random.choices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddbc99d9f767bd80f35d1a6405b730fd",
     "grade": false,
     "grade_id": "cell-8abbd59bb4c0e2b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text_mle(model_counts):\n",
    "    \"\"\"\n",
    "    this function takes in the n-gram model and produces text until the end symbol is generated.\n",
    "    \n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    OUTPUT:\n",
    "    sentence - a sentence generated by model as a list of tokens.\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts))) \n",
    "    n = context_length + 1 # the ngram length n\n",
    "    start = tuple(['<s>']*(n-1))\n",
    "    end = '</s>'\n",
    "    sentence = list(start)\n",
    "    \n",
    "    while sentence[-1] != end:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    #Strip the padding:\n",
    "    sentence = sentence[n-1:-1]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b9f830c8beb0e8f08457052181c98f",
     "grade": false,
     "grade_id": "cell-f3cbfe3afa729f52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "print(generate_text_mle(dummy_model_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42a38b26f9b52ba9c0a526a27dddd30c",
     "grade": false,
     "grade_id": "cell-842564ff5b2f8a3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1.2 Additive smoothing generation\n",
    "\n",
    "Now we'll implement generating text from a smoothed model. In this case, you can use the modified counts of each n-gram as the weights. Note that in this case, you need to know the vocabulary of the model, so it is given to the function as an argument.\n",
    "\n",
    "NOTE: vocab should include the end of sentence tag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7fb6f70196259e640735d0fab09e1ab",
     "grade": false,
     "grade_id": "cell-5eed9b8c9273ba2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text_smoothed(model_counts, delta, vocab):\n",
    "    \"\"\"\n",
    "    this function takes in the n-gram model and produces text until the end symbol is generated.\n",
    "    \n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    delta - the delta value to be added to the counts\n",
    "    vocab - a set of words in the vocabulary\n",
    "    OUTPUT:\n",
    "    sentence - a sentence generated by model as a list of tokens.\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts))) \n",
    "    n = context_length + 1 # the ngram length n\n",
    "    start = tuple(['<s>']*(context_length))\n",
    "    end = '</s>'\n",
    "    sentence = list(start)\n",
    "    \n",
    "    while sentence[-1] != end:\n",
    "        curr_context_counts = {}\n",
    "        try:\n",
    "            curr_context_counts.update(model_counts[tuple(sentence[-context_length:])])\n",
    "        except KeyError: # This context was not seen once\n",
    "            pass\n",
    "        # NEXT: Add smoothing delta, then generate the next token.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    #Strip the padding:\n",
    "    sentence = sentence[context_length:-1]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea3a1ec71262d283b1790817a5452cb",
     "grade": false,
     "grade_id": "cell-777b5bd912cdae3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "\n",
    "print(generate_text_smoothed(dummy_model_3, \n",
    "                       delta=0.1,\n",
    "                       vocab=dummy_model_vocab))\n",
    "\n",
    "print(generate_text_smoothed(dummy_model_3,\n",
    "                       delta=0.4,\n",
    "                       vocab=dummy_model_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7231a9f7e4da53add175efb64bd6fc7",
     "grade": false,
     "grade_id": "cell-9eb98b5cfa7e7d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 6\n",
    "## Working with real text data\n",
    "\n",
    "Let's try to model some real data. In this assignment we are working with Jane Austen's novel Pride and Prejudice, which is provided Project Gutenberg [here](http://www.gutenberg.org/ebooks/1342). In the version in the coursedata directory, the text has had some ebook disclaimers removed, as they were certainly not written by Jane Austen.\n",
    "\n",
    "First, we'll tokenize the corpus. As you remember from the Intro assignment, tokenization, processing raw text into a usable format, is not trivial. Here, we'll again use tokenizers from nltk. \n",
    "\n",
    "We'll need to split the data into sentences. The nltk's PunktSentenceTokenizer uses \"an unsupervised algorithm\" to find sentence boundaries, and a pretrained version is available for English. You may be prompted to download it. Though typically sentence boundaries are marked with hard punctuation, particularly the full stop is also found elsewhere, so the task is not at all trivial.\n",
    "\n",
    "To tokenize the sentences, we'll use the same TreebankWordTokenizer as in the Intro. This will not for example lowercase the data, which might not be ideal for all applications, but here we have no particular use case in mind, so the TreebankWordTokenizer will do. Note that this means that capitalized and lowercase versions of words are now modeled as two separate things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60600ebda6b8627801d08f7b51137ee5",
     "grade": false,
     "grade_id": "cell-11c88e1929d9a7e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "import nltk.util\n",
    "\n",
    "with open(\"/coursedata/pride-and-prejudice.txt\") as fi:\n",
    "    janeausten_raw = fi.read()\n",
    "\n",
    "sentence_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer() # Splits a long text into sentences\n",
    "word_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "word_detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer() # Reverses tokenization\n",
    "janeausten_sentences = list(sentence_tokenizer.sentences_from_text(janeausten_raw))\n",
    "janeausten_tokenized = list(word_tokenizer.tokenize(sentence) for sentence in janeausten_sentences)\n",
    "\n",
    "print(\"Let's see an example sentence:\")\n",
    "print(janeausten_tokenized[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfd227f7ab5f2323505feb3504d5367",
     "grade": false,
     "grade_id": "cell-9195a2c8bf79ed63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train-test split\n",
    "\n",
    "We'll divide the data into training and test sets. The training set is used to create the model, and the test set used for evaluation. Here, we'll randomly select 10% of the sentences to be used as the test set. This way, the training data matches the test data very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84face6a17e0802512c5090059e341a8",
     "grade": false,
     "grade_id": "cell-2bc1167478df6c70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_split_index = round(0.9 * len(janeausten_tokenized))\n",
    "random.seed(808)\n",
    "janeausten_tokenized_randperm = random.sample(janeausten_tokenized, len(janeausten_tokenized))\n",
    "janeausten_train = janeausten_tokenized_randperm[:test_split_index]\n",
    "janeausten_test = janeausten_tokenized_randperm[test_split_index:]\n",
    "\n",
    "print(\"Training set has\", len(janeausten_train), \"sentences, and the test set\", len(janeausten_test), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ffc0466118813b42b1780d82d511339",
     "grade": false,
     "grade_id": "cell-04de4ab989145dbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "If we just use the full vocabulary of the training data, we will include some rare words, \n",
    "while excluding other words. One way to deal with this is to limit the vocabulary to some most common words. \n",
    "Everything else will be an unknown token, dealt with, together, as the \"&lt;UNK&gt;\" token.\n",
    "\n",
    "\n",
    "We will limit the vocabulary to the 1000 most common words in the training set. Additionally we include the end of sentence tag and the unknown word tag, because we want to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aecc45790876e5c491235dc8e5c6c5fa",
     "grade": false,
     "grade_id": "cell-e8f1e6a116027d92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "janeausten_unigram_counts = Counter(itertools.chain.from_iterable(janeausten_train)).most_common()\n",
    "\n",
    "print(\"The 10 most common tokens:\\n\", \"\\n\".join(word for word, freq in janeausten_unigram_counts[:10]))\n",
    "print()\n",
    "print(\"The 990-1000 most common tokens:\\n\", \"\\n\".join(word for word, freq in janeausten_unigram_counts[990:1000]))\n",
    "\n",
    "janeausten_vocab_1k = set(word for word, freq in janeausten_unigram_counts[:1000]) | {\"</s>\", \"<unk>\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fe7e8aa3bd1442a7c67cde952d8255c",
     "grade": false,
     "grade_id": "cell-2391bf7bd77a1703",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.1  replace tokens not in the vocabulary\n",
    "\n",
    "Implement the replaing of tokens which are not in the vocabulary (out-of-vocabulary words, OOVs). They are to be replaced with the unknown token \"&lt;unk&gt;\". Fill in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdd5ed274e0521eb168b419f74dc1295",
     "grade": false,
     "grade_id": "cell-d02c0b6e2a7711d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def replace_oovs(vocab, data, unk=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    vocab: set of tokens\n",
    "    data: list of lists, i.e. list of sentences, which are lists of tokens\n",
    "    unk: token to replace tokens which are not in the vocabulary\n",
    "    \n",
    "    This function replaces all tokens not in the vocabulary with the unknown token\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return data_oovs_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2085c657f08ec6a36928e8f2358e9fa0",
     "grade": true,
     "grade_id": "cell-4daf2001dce04789",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(replace_oovs({\"a\",\"b\",\"c\"}, [[\"a\", \"b\"],[\"a\",\"b\",\"c\",\"d\"]]), [['a', 'b'], ['a', 'b', 'c', '<unk>']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f86cc9a1954c8c57db6b8672ef907491",
     "grade": false,
     "grade_id": "cell-90bdfe5f57bd9bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll apply the filter to the Jane Austen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1904425dd068dffb19cdd1bdfe2830",
     "grade": false,
     "grade_id": "cell-9af163c379d54e33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "janeausten_train_1k = replace_oovs(janeausten_vocab_1k, janeausten_train)\n",
    "janeausten_test_1k = replace_oovs(janeausten_vocab_1k, janeausten_test)\n",
    "\n",
    "print(\"Let's see an example:\")\n",
    "print(word_detokenizer.detokenize(janeausten_train_1k[106]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7abd58a17c9106e64102588393695791",
     "grade": false,
     "grade_id": "cell-e59be86cd2f39931",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### MLE model\n",
    "\n",
    "First we'll train a maximum likelihood estimate trigram model. Now that we're using a larger dataset, the estimation might take some time (a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3802a0311ae158e664f3588c2697d98",
     "grade": false,
     "grade_id": "cell-25b108227c537bec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's pad the training corpus \n",
    "janeausten_train_1k_padded = [pad(sent, n=3) for sent in janeausten_train_1k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d2e6f73d7b0cba8ceef79b5082c5bc3",
     "grade": false,
     "grade_id": "cell-d42515202a1de004",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# now let's get the n-gram counts\n",
    "janeausten_train_1k_counts = get_counts(janeausten_train_1k_padded, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f439cf84a33c7658adf607d4900a7c60",
     "grade": false,
     "grade_id": "cell-9ea7418301cb5887",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, even though any unseen token is accounted for by the unknown token, we still get infinite perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21dc66704f249a8abc0d2b98caf4b54",
     "grade": false,
     "grade_id": "cell-f79809498aa30977",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea6fe048ac0c418eece72f22b7d0735f",
     "grade": false,
     "grade_id": "cell-c35757b74baa6caa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Smoothed model\n",
    "\n",
    "Now, we'll smooth our counts with a few different delta values and look at perplexities. This will also take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28339abaf47d378653d0d2a9f6dd66ab",
     "grade": false,
     "grade_id": "cell-6ce3158bd7c64819",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we can get some type of proper perplexity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_smoothed, delta=1, vocab=janeausten_vocab_1k))\n",
    "print(perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_smoothed, delta=0.01, vocab=janeausten_vocab_1k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7a400f0b9a5894dc5e40d5464d1d8a9",
     "grade": false,
     "grade_id": "cell-1c8a5cc764ce91bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TASK: Generate sentences, comment on differences\n",
    "\n",
    "Let's see what kinds of sentences do the two types of models generate.\n",
    "Generate some sentences, and comment on the differences.\n",
    "\n",
    "- Why do the smoothed models produce such long walls of text?\n",
    "    - If we backed off to lower order models, would that help?\n",
    "- Which model generates text, that looks most like the original? Why?\n",
    "- Which model do you think would be the best language model for e.g. optical character recognition of Jane Austen's hand written notes, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dee88d0b2988bcd2e8eb0b4c4089c51",
     "grade": false,
     "grade_id": "cell-37cc13550d54f074",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sentences from the MLE model:\")\n",
    "for i in range(5):\n",
    "    print(word_detokenizer.detokenize(generate_text_mle(janeausten_train_1k_counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca1a4c2457d8eeac55593cf1629a63af",
     "grade": false,
     "grade_id": "cell-90e43a931cdac577",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"A sentence from the low delta smoothed model:\")\n",
    "print(word_detokenizer.detokenize(generate_text_smoothed(janeausten_train_1k_counts, delta=0.01, vocab=janeausten_vocab_1k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8102b602cc51129a79e1e0be2c4a0663",
     "grade": false,
     "grade_id": "cell-1be4b0fb3303dfb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"A sentence from the high delta smoothed model:\")\n",
    "print(word_detokenizer.detokenize(generate_text_smoothed(janeausten_train_1k_counts, delta=1.0, vocab=janeausten_vocab_1k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e67ad71a37aa5fbae174775f8b30a687",
     "grade": true,
     "grade_id": "cell-a48c96187f0db2fb",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
