{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea93192c678d460c28339603886d44b",
     "grade": false,
     "grade_id": "cell-a74ccc5dcde03d2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 5: Morphological segmentation\n",
    "\n",
    "# Released: 12.02.2020\n",
    "# Deadline: 25.03.2020 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21fb042b23ddfecb27b4253c5645de38",
     "grade": false,
     "grade_id": "cell-11a4ebb4eedeadb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing this assignment, you'll learn how to tokenize text into subwords using BPE algorithm\n",
    "\n",
    "KEYWORDS:\n",
    "* subwords\n",
    "* BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceaf236fe686c7e56a8c12440e4b9eac",
     "grade": false,
     "grade_id": "cell-bd0631ed211d7cca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We've already talked about the problem of segmenting text into appropriate units (tokenization). Back then, we were considering words as those units, but there are other units that you should probably explore as well: characters and **subwords**. In this assignment, we're going to focus on **tokenization into subwords**.\n",
    "\n",
    "The motivation to segmenting words further into smaller elements comes from morphology, where such elements are called **morphemes**. A **morpheme** is defined as the smallest meaning-bearing unit of a language. For example, the word *unpredictable* contains three morphemes: *un*, *predict* and *able*. As you can see, morphemes are not unique to one word, they are elements that are regularly seen in other words too. For example, *un* in *unhappy*, *predict* in *predictive*, and *able* in *comfortable*. Thus, just as sentences are constructed from words, words are constructed from morphemes. \n",
    "\n",
    "Segmenting into morphemes (especially in languages with rich morphology) helps to avoid the problem of out-of-vocabulary (OOV) words in text corpora. For example, if our training corpus contains *cool*, *cool-est* and *dumb-er* when the new word *cooler* comes in, it can be analyzed into *cool-er*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a681812a4a6ac93e80ca737581441471",
     "grade": false,
     "grade_id": "cell-2cbd322d57c4b67a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1\n",
    "# BPE\n",
    "One approach to tokenization into subwords is based on the **byte pair encoding** (**BPE**) algorithm for text compression. It iteratively merges frequent pairs of characters forming new subwords. The intuition here is that morphemes are frequently repeated substrings, so this method should merge symbols into them instead of into some random meaningless character sequences. The algorithm is applied only inside words (there is no merges across word boundaries). \n",
    "\n",
    "**BPE** algorithm begins with its vocabulary being a set of characters seen in the training corpus. Each word in the corpus is represented as a sequence of characters plus a special end-of-word symbol '_'. At each iteration step $k_i$, the algorithm counts the number of symbol pairs, finds the most frequent pair ['A','B'] and replaces it with the new merged symbol ‘AB’. The algorithm stops when it's done $k$ merges ($k$ is a parameter of the algorithm). The algorithm begins with the set of symbols equal to the set of characters. The resulting symbol set should have the original set of characters plus $k$ new symbols. \n",
    "\n",
    "To learn segmentations with **BPE**, you should take the following steps:\n",
    "* STEP 1: tokenize a training corpus into words and collect frequency statistics of word tokens in the training corpus.\n",
    "* STEP 2: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 3: count the frequencies of symbol pairs.\n",
    "* STEP 4: replace every occurance of the most frequent pair ['A','B'] with the new merged symbol 'AB'.\n",
    "* STEP 5: repeat STEPs 3-4 k-1 times more.\n",
    "\n",
    "To segment a test corpus with learned segmentations, you shoud:\n",
    "* STEP 6: tokenize a test corpus into words (with the same tokenisation algorith as in trainig).\n",
    "* STEP 7: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 8: for every word apply each merge operation in the order they were learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcec13d6e240bd31b139edb27645532d",
     "grade": false,
     "grade_id": "cell-fc78b008631ec01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP 1\n",
    "### Tokenize into words and collect frequencies\n",
    "## 1.1\n",
    "Write a function that reads a text from a file, tokenizes it into words by whitespaces and collects the word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c05c43def4802a63b55faf26e93ab7",
     "grade": false,
     "grade_id": "cell-1b45cebfbb7868fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def collect_word_counts(file_name):\n",
    "    \"\"\"\n",
    "    this function takes in a path to a text file, reads the file,\n",
    "    tokenizes it into words, and then counts their frequencies.\n",
    "    \n",
    "    INPUT:\n",
    "    file_name - a path to a training corpus as a string\n",
    "    OUTPUT:\n",
    "    word_counts - a frequency dictionary of words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    word_counts = {}\n",
    "\n",
    "    file = open(file_name, 'r').read().split()\n",
    "\n",
    "    for word in file:\n",
    "        if word in word_counts.keys():\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7aaabd4794b7ef9172a75498ec4c68",
     "grade": true,
     "grade_id": "cell-ac8d751900ffa327",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/SUBS/dummy_corpus.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(collect_word_counts(dummy_corpus_path)), dict)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['low'], 5)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['lowest'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['new'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['newer'], 6)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['wider'], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60fda7c049421e50ed351d9bf40e464c",
     "grade": false,
     "grade_id": "cell-c92b7f1ca22e062e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP 2\n",
    "### Convert words into strings of characters\n",
    "## 1.2\n",
    "Now, represent each word in your frequency dictionary as a tuple of characters plus a special end-of-word symbol '_'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66a1ddfef19da0ff5d26e9c22f31f7d6",
     "grade": false,
     "grade_id": "cell-e5f794a491bb8695",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_chars(vocab):\n",
    "    \"\"\"\n",
    "    this function takes in a frequeny dictionary of words in the trainin corpus, \n",
    "    and converts the key words to a tuple of characters plus a special end-of-word symbol '_'.\n",
    "    \n",
    "    INPUT:\n",
    "        vocab - a frequency dictionary of words\n",
    "    OUTPUT:\n",
    "        separated_vocab - a frequency dictionary of words \n",
    "        represented as a tuple of characters plus a special end-of-word symbol '_'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    separated_vocab = {}\n",
    "    for key in vocab.keys():\n",
    "        separated_vocab[tuple(key + '_')] = vocab[key]\n",
    "    return separated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36ea250216fc175dc4ff72a07e2b9d5f",
     "grade": true,
     "grade_id": "cell-10d1967f2c149a8b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab = {'low': 5, 'lowest': 2, 'newer': 6, 'wider': 3, 'new': 2}\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(convert_to_chars(dummy_freq_vocab)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(convert_to_chars(dummy_freq_vocab).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(convert_to_chars(dummy_freq_vocab)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', '_')], 5)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', 'e', 's', 't', '_')], 2)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('n', 'e', 'w', 'e', 'r', '_')], 6)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('w', 'i', 'd', 'e', 'r', '_')], 3)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('n', 'e', 'w', '_')], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40d55f1e2e1c44536ed64f12ec4dd336",
     "grade": false,
     "grade_id": "cell-48322d22d20d67b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP 3\n",
    "### Collect frequencies of symbol pairs\n",
    "## 1.3\n",
    "Write a function that takes in a frequency dictionary, where keys are words represented as tuples of symbols, and outputs the most frequent pair of symbols in the corpus. In the case, when there are several pairs with the same frequency, return the pair that is earlier alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cacdbf6da10c85a47041b93c37e0c742",
     "grade": false,
     "grade_id": "cell-e2a9fb6eb0db7f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_the_pair_to_merge(vocab_as_symbols):\n",
    "    \"\"\"\n",
    "    this function takes in a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "    and outputs the most frequent pair of symbols in the corpus\n",
    "    \n",
    "    INPUT:\n",
    "        vocab_as_symbols - a frequency dictionary, where keys are words represented as tuples of symbols\n",
    "    OUTPUT:\n",
    "        merge_pair - the most frequent pair of symbols, a pair to merge (a tuple)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pair_freq = {}\n",
    "    for key in vocab_as_symbols.keys():\n",
    "        for i in range(len(key) - 1):\n",
    "            if key[i:i + 2] in pair_freq.keys():\n",
    "                pair_freq[key[i:i + 2]] += vocab_as_symbols[key]\n",
    "            else:\n",
    "                pair_freq[key[i:i + 2]] = vocab_as_symbols[key]\n",
    "    merge_pair = max(pair_freq.keys(), key=(lambda k: pair_freq[k]))\n",
    "    return merge_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68a5a9ec698630255cc8caa1af5a7a4c",
     "grade": true,
     "grade_id": "cell-74bc7d2d9ec1da10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab_as_symbols = {('l', 'o', 'w', '_'): 5,\n",
    "                               ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                               ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                               ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                               ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "# check that the output of the function is a tuple\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)), tuple)\n",
    "# check that the output of the function is a tuple of strings\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)[0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols), ('e', 'r'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f799bd13e4d2623785c93c9f8f7766",
     "grade": false,
     "grade_id": "cell-eb3e7bd4a0d2d372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP 4\n",
    "### Merge the most frequent pair\n",
    "## 1.4\n",
    "Write a function that takes in a pair of symbols to merge and a frequency dictionary where words are represented as tuples of symbols, and returns a frequency dictionary, where words are still represented as tuples of symbols, but the most frequnt pair was merged in every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81a798c34231da64beea6ed78893ad73",
     "grade": false,
     "grade_id": "cell-7f9d1e7549722c1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def merge(vocab_as_symbols, merge_pair):\n",
    "    \"\"\"\n",
    "    this function takes in a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "    and outputs the most frequent pair of symbols in the corpus\n",
    "    \n",
    "    INPUT:\n",
    "        merge_pair - a pair to merge (a tuple)\n",
    "        vocab_as_symbols - a frequency dictionary, where keys are words represented as tuples of symbols\n",
    "    OUTPUT:\n",
    "        new_vocab_as_symbols - a frequency dictionary, where keys are words represented as tuples of symbols,\n",
    "        with the given pair represented as a new symbol (concatenated pair)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    new_vocab = {}\n",
    "\n",
    "    def find_sub_idx(test_list, repl_list, start=0):\n",
    "        length = len(repl_list)\n",
    "        for idx in range(start, len(test_list)):\n",
    "            if test_list[idx: idx + length] == repl_list:\n",
    "                return idx, idx + length\n",
    "\n",
    "    def replace_sub(test_list, repl_list, new_list):\n",
    "        length = len(new_list)\n",
    "        idx = 0\n",
    "        for start, end in iter(lambda: find_sub_idx(test_list, repl_list, idx), None):\n",
    "            test_list[start: end] = new_list\n",
    "            idx = start + length\n",
    "        return test_list\n",
    "\n",
    "    for key in vocab_as_symbols.keys():\n",
    "        new_key = replace_sub(list(key), list(merge_pair), [''.join(merge_pair)])\n",
    "        new_vocab[tuple(new_key)] = vocab_as_symbols[key]\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9429172cf55d3793bc9bcbe5d8a1374c",
     "grade": true,
     "grade_id": "cell-e71ea4c2a2aee8c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_as_symbols = {('l', 'o', 'w', '_'): 5,\n",
    "                               ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                               ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                               ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                               ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "dummy_pair = ('e', 'r')\n",
    "\n",
    "# check that the output of the function is a dictionary\n",
    "assert_equal(type(merge(dummy_vocab_as_symbols, dummy_pair)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(merge(dummy_vocab_as_symbols, dummy_pair).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(merge(dummy_vocab_as_symbols, dummy_pair)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the pair was merged everywhere\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols, dummy_pair).keys()), [('l', 'o', 'w', '_'), \n",
    "                                                                    ('l', 'o', 'w', 'e', 's', 't', '_'), \n",
    "                                                                    ('n', 'e', 'w', 'er', '_'), \n",
    "                                                                    ('w', 'i', 'd', 'er', '_'), \n",
    "                                                                    ('n', 'e', 'w', '_')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6b1b77c611da37190858f2f7798d0cb",
     "grade": false,
     "grade_id": "cell-ddba734f24516d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP 1-5\n",
    "### Combine all functions and make all k merges\n",
    "## 1.5\n",
    "Now let's combine steps 1-5 into a function, that learns $k$ BPE merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01623872632ef673fd9a5d6ec2a329a4",
     "grade": false,
     "grade_id": "cell-ccfb0b9de81867b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def learn_BPE_merges(file_name, k):\n",
    "    \"\"\"\n",
    "    this function takes in a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "    and outputs the most frequent pair of symbols in the corpus\n",
    "    \n",
    "    INPUT:\n",
    "        file_name - a path to a training corpus as a string\n",
    "        k - a number of merges to be learned\n",
    "    OUTPUT:\n",
    "        merges - a list of k merges in the order they were learned\n",
    "        one merge is a tuple of two most frequent symbols at step k\n",
    "    \"\"\"\n",
    "    \n",
    "    merges = []\n",
    "    # YOUR CODE HERE\n",
    "    freq_dict = collect_word_counts(file_name)\n",
    "    separated_vocab = convert_to_chars(freq_dict)\n",
    "\n",
    "    for i in range(k):\n",
    "        merges.append(get_the_pair_to_merge(separated_vocab))\n",
    "        separated_vocab = merge(separated_vocab, merges[i])\n",
    "\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "159351c251f6a62350bb579297643d3c",
     "grade": true,
     "grade_id": "cell-5bfcbdbe63a3fdff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Lists differ: [('e'[18 chars]), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo',[31 chars]'_')] != [('e'[18 chars]), ('e', 'w'), ('n', 'ew'), ('l', 'o'), ('lo',[31 chars]'_')]\n\nFirst differing element 2:\n('n', 'e')\n('e', 'w')\n\n  [('e', 'r'),\n   ('er', '_'),\n-  ('n', 'e'),\n-  ('ne', 'w'),\n?    -\n\n+  ('e', 'w'),\n+  ('n', 'ew'),\n   ('l', 'o'),\n   ('lo', 'w'),\n   ('new', 'er_'),\n   ('low', '_')]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2ed1928e78c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                      \u001b[0;34m(\u001b[0m\u001b[0;34m'lo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                      \u001b[0;34m(\u001b[0m\u001b[0;34m'new'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'er_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                      ('low', '_')])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36massertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m    838\u001b[0m         \u001b[0massertion_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getAssertEqualityFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0massertion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertNotEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36massertListEqual\u001b[0;34m(self, list1, list2, msg)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \"\"\"\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertSequenceEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertTupleEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36massertSequenceEqual\u001b[0;34m(self, seq1, seq2, msg, seq_type)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mstandardMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncateMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardMsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_truncateMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/unittest/case.py\u001b[0m in \u001b[0;36mfail\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;34m\"\"\"Fail immediately, with the given message.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertFalse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Lists differ: [('e'[18 chars]), ('n', 'e'), ('ne', 'w'), ('l', 'o'), ('lo',[31 chars]'_')] != [('e'[18 chars]), ('e', 'w'), ('n', 'ew'), ('l', 'o'), ('lo',[31 chars]'_')]\n\nFirst differing element 2:\n('n', 'e')\n('e', 'w')\n\n  [('e', 'r'),\n   ('er', '_'),\n-  ('n', 'e'),\n-  ('ne', 'w'),\n?    -\n\n+  ('e', 'w'),\n+  ('n', 'ew'),\n   ('l', 'o'),\n   ('lo', 'w'),\n   ('new', 'er_'),\n   ('low', '_')]"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/SUBS/dummy_corpus.txt\"\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)), list)\n",
    "# check that the output of the function is a list of tuples\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0]), tuple)\n",
    "# check that the output of the function is a list of tuples of strings\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0][0]), str)\n",
    "#check that there are exactly k merges\n",
    "assert_equal(len(learn_BPE_merges(dummy_corpus_path, 10)), 10)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the pair was merged everywhere\n",
    "assert_equal(learn_BPE_merges(dummy_corpus_path, 8), [('e', 'r'),\n",
    "                                                     ('er', '_'),\n",
    "                                                     ('e', 'w'),\n",
    "                                                     ('n', 'ew'),\n",
    "                                                     ('l', 'o'),\n",
    "                                                     ('lo', 'w'),\n",
    "                                                     ('new', 'er_'),\n",
    "                                                     ('low', '_')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS 6-8\n",
    "### Segment a test corpus\n",
    "## 1.6\n",
    "Well, now we can apply what we've learned to segment any text. Write a function that reads the test corpus, and applies the merges we've learned. Note that you will probably need to adapt your previous functions. For example, we don't need to count the word frequencies anymore, since they don't play any role here.\n",
    "\n",
    "Note: don't forget to get rid of the special end-of-word symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125dc950c1abfce93a86299c0a2836e4",
     "grade": false,
     "grade_id": "cell-bc393db75acac1eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(file_name, merges):\n",
    "    \"\"\"\n",
    "    this function takes in a path to a text file, reads the file,\n",
    "    and tokenizes it into subwords in accorance with the learned BPE merges.\n",
    "    \n",
    "    INPUT:\n",
    "        file_name - a path to a text as a string\n",
    "        merges - a list of k merges in the order they were learned\n",
    "        \n",
    "    OUTPUT:\n",
    "        segmented_text - a texted segmented with BPE. It's a list of words\n",
    "        where each word is a string, with its segments separated by whitespaces\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    file = open(file_name, 'r').read().split()\n",
    "    vocab_dict = {key: key for key in file}\n",
    "    vocab_as_symbols = convert_to_chars(vocab_dict)\n",
    "    segmented_text = []\n",
    "    for pair in merges:\n",
    "        vocab_as_symbols = merge(vocab_as_symbols, pair)\n",
    "    keys = list(vocab_as_symbols.keys())\n",
    "    for i in range(len(keys)):\n",
    "        seperator = ' '\n",
    "        keys[i] = seperator.join(keys[i]).strip(' _')\n",
    "    vals = list(vocab_as_symbols.values())\n",
    "    for word in file:\n",
    "        segmented_text.append(keys[vals.index(word)])\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7675a9785d264725b0eec51939cd57cf",
     "grade": true,
     "grade_id": "cell-6d91df8425edb415",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/SUBS/dummy_corpus.txt\"\n",
    "\n",
    "dummy_merges = [('e', 'r'),\n",
    "                 ('er', '_'),\n",
    "                 ('e', 'w'),\n",
    "                 ('n', 'ew'),\n",
    "                 ('l', 'o'),\n",
    "                 ('lo', 'w')]\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(segment_text(dummy_corpus_path, dummy_merges)), list)\n",
    "# check that the output of the function is a list of strings\n",
    "assert_equal(type(segment_text(dummy_corpus_path, dummy_merges)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the train corpus is segmented the way it should:\n",
    "\n",
    "assert_equal(segment_text(dummy_corpus_path, dummy_merges), ['low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low e s t',\n",
    "                                                             'low e s t',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'w i d er',\n",
    "                                                             'w i d er',\n",
    "                                                             'w i d er',\n",
    "                                                             'new',\n",
    "                                                             'new'])\n",
    "\n",
    "\n",
    "# check that the test corpus is segmented the way it should:\n",
    "dummy_test_path = \"/coursedata/SUBS/dummy_test_corpus.txt\"\n",
    "assert_equal(segment_text(dummy_test_path, dummy_merges), ['low er', 'c o o l er'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c416c4c99ecd2662fc9098dcc98120d",
     "grade": false,
     "grade_id": "cell-3b41d5204748e381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2\n",
    "# ANALYZE SEGMENTATIONS\n",
    "### Count word OOV\n",
    "## 2.1\n",
    "Now that we've done with the algorithm, let's see if it will actually help us with the OOV problem.\n",
    "Let's use the same corpus as we did in the previous assignment (POS-tagging). We've randomly shuffled the sentences and split the corpus in roughly half. One half will be our training example, and another will be our test example.\n",
    "\n",
    "Analyse:\n",
    "1. The number of words in the vocabulry of the training corpus \n",
    "2. The number of words in the vocabulry of the test corpus\n",
    "3. The number of words in the test corpus, that were not seen in the training corpus\n",
    "\n",
    "Run the cell below, to collect the tokenizes corpora, type in the answer in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "414b09a2ac79f1fe7d6e6e96b1ef57c4",
     "grade": false,
     "grade_id": "cell-a3ce2430b488bbe3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f = open(\"/coursedata/SUBS/gum_train.txt\", 'r')\n",
    "train = f.read()\n",
    "f.close()\n",
    "train = train.split()\n",
    "\n",
    "f = open(\"/coursedata/SUBS/gum_test.txt\", 'r')\n",
    "test = f.read()\n",
    "f.close()\n",
    "test = test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae17365dbddb2ed3daca68b28e6513b",
     "grade": false,
     "grade_id": "cell-6b5793a004cb3f7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_train_vocab = 234\n",
    "len_of_train_vocab = 8677\n",
    "\n",
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_test_vocab = 234\n",
    "len_of_test_vocab = 8770\n",
    "\n",
    "# type in the answer as a number between 0 and 1\n",
    "# For example:\n",
    "# oov_portion = 0.5\n",
    "oov_portion = 5035 / len_of_train_vocab\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac57c39d9d0a68fd29526d855f5a0503",
     "grade": true,
     "grade_id": "cell-dc2b01fa475e484e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a08747d1589ba42abaa51a9f897a451",
     "grade": false,
     "grade_id": "cell-586822c916ee5799",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Count subword OOV\n",
    "## 2.2\n",
    "Let's compare the OOV numbers we've got in the case where the text is tokenized by words and the case when it's tokenized by subwords. \n",
    "\n",
    "Learn 5000 BPE segmentation on the train data, then segment both corpora and compare the vocabulary numbers again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "460b0d3fad500b1f4a1149f14b1822b3",
     "grade": false,
     "grade_id": "cell-10da1fe631f3755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "merges = learn_BPE_merges(\"/coursedata/SUBS/gum_train.txt\", 5000)\n",
    "segmented_train= segment_text(\"/coursedata/SUBS/gum_train.txt\", merges)\n",
    "segmented_test = segment_text(\"/coursedata/SUBS/gum_test.txt\", merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aec9d10c00e9b2f272f8ab1bea6aaf1c",
     "grade": false,
     "grade_id": "cell-e5d749c71ee045a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_train_sub_vocab = 234\n",
    "len_of_train_sub_vocab = 4306\n",
    "\n",
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_test_sub_vocab = 234\n",
    "len_of_test_sub_vocab = 3643\n",
    "\n",
    "# type in the answer as a number between 0 and 1\n",
    "# For example:\n",
    "# oov_sub_portion = 0.5\n",
    "oov_sub_portion = 137 / len_of_train_sub_vocab\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9addaa1428730752a74444025beb1067",
     "grade": true,
     "grade_id": "cell-c29d08ffc3d29073",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47100764fcd947db18a041354b9a7748",
     "grade": false,
     "grade_id": "cell-e9c3e0cf6893442b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Does the segmentation make sense?\n",
    "## 2.3\n",
    "Now let's look a bit closer at the subwords that we've learned.\n",
    "\n",
    "1. What are the top 10 most frequent subwords in the test corpus? (in decending order)\n",
    "2. What are the top 5 longest subwords in the test corpus?\n",
    "3. What are the top 5 most frequent legths of subwords in the test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b72f4d3acf0ed7fbfaf448f35349083",
     "grade": false,
     "grade_id": "cell-e30eb3fbd26f4157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as a list of strings\n",
    "# For example:\n",
    "# top_10_by_freq = ['a','b'...]\n",
    "top_10_by_freq = ['e_', 's_', 'th', 't_', 'd_', 'in', 'an', 'er', 'y_', ',_']\n",
    "\n",
    "# type in the answer as a list of strings\n",
    "# For example:\n",
    "# top_5_by_len = ['a','b'...]\n",
    "top_5_by_len = ['incomprehensible_', 'representative_', 'discrimination_', 'transportation_', 'photographers_']\n",
    "\n",
    "# type in the answer as a list of integers\n",
    "# For example:\n",
    "# top_5_freqs_of_lens= [1,2,3...]\n",
    "top_5_freqs_of_lens = [3, 4, 5, 6, 7]\n",
    "\n",
    "# Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f51c9b98583f1ce9f5fa38f23b26282a",
     "grade": true,
     "grade_id": "cell-e7786799f7aeacdc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd5f9195e82cac6279e0b4d7bfdf0ad1",
     "grade": false,
     "grade_id": "cell-f67771f63a0e8969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Your thoughts\n",
    "## 2.4\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Describe what will happen if you change the k parameter? How to find a good number for k?\n",
    "\n",
    "2. What are the possible NLP applications that can benefit from the tokenisation into subwords? \n",
    "\n",
    "3. How the OOV number for our data can be lowered further without changing anything in the segmentation procedure (k stays the same)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134371f8aa1deaa508fec6d0910e6708",
     "grade": true,
     "grade_id": "cell-18c675e80eea69ed",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
